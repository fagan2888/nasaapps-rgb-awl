{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import gc\n",
    "import re\n",
    "import operator \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "from sklearn import model_selection\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import cv2\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from keras.preprocessing import image as Im\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Input, Conv2D, BatchNormalization, MaxPooling2D, UpSampling2D, Conv2DTranspose, Dense, Activation, Lambda, Reshape, Flatten\n",
    "from keras.models import Model, load_model\n",
    "\n",
    "from keras import backend as K\n",
    "import argparse\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv2D, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten, Masking\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras.models import Model, load_model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras import backend as K\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint,  Callback, EarlyStopping, ReduceLROnPlateau\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH='./selector_output/train'\n",
    "TEST_PATH='./selector_output/val'\n",
    "UNLABELLED_PATH=\"./selector_output/unlab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 120 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#FOR VALIDATION DATA\n",
    "gen = ImageDataGenerator()\n",
    "data = gen.flow_from_directory(TRAIN_PATH, target_size=(128, 128), class_mode='binary', batch_size=128, shuffle=False)\n",
    "data.reset()\n",
    "images, classes = data.next()\n",
    "img_ary = np.array(images.astype('float'))\n",
    "cls_ary = np.array(classes)\n",
    "rng = int(data.samples // 128)\n",
    "\n",
    "for _ in range(rng):\n",
    "    \n",
    "    images, classes = data.next()\n",
    "    img_ary = np.vstack((img_ary, images.astype('float')))\n",
    "    cls_ary = np.hstack((cls_ary, classes))\n",
    "    print (_)\n",
    "\n",
    "#channel_means = img_ary.mean(axis=(0,1,2)).astype('float')\n",
    "#img_ary = img_ary - channel_means\n",
    "    \n",
    "train_X = img_ary\n",
    "#train_X=train_X/train_X.max()\n",
    "train_y = cls_ary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1787 images belonging to 2 classes.\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "#FOR VALIDATION DATA\n",
    "gen = ImageDataGenerator()\n",
    "data = gen.flow_from_directory(TEST_PATH, target_size=(128, 128), class_mode='binary', batch_size=128, shuffle=False)\n",
    "data.reset()\n",
    "images, classes = data.next()\n",
    "img_ary = np.array(images.astype('float'))\n",
    "cls_ary = np.array(classes)\n",
    "rng = int(data.samples // 128)\n",
    "\n",
    "for _ in range(rng):\n",
    "    \n",
    "    images, classes = data.next()\n",
    "    img_ary = np.vstack((img_ary, images.astype('float')))\n",
    "    cls_ary = np.hstack((cls_ary, classes))\n",
    "    print (_)\n",
    "\n",
    "#channel_means = img_ary.mean(axis=(0,1,2)).astype('float')\n",
    "#img_ary = img_ary - channel_means\n",
    "    \n",
    "train_X_val = img_ary\n",
    "#train_X_val=train_X_val/train_X_val.max()\n",
    "train_y_val = cls_ary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_val.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'person': 0, 'worker': 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9852 images belonging to 1 classes.\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#FOR VALIDATION DATA\n",
    "gen = ImageDataGenerator()\n",
    "data = gen.flow_from_directory(UNLABELLED_PATH, target_size=(128, 128), class_mode=None, batch_size=128, shuffle=False)\n",
    "data.reset()\n",
    "images = data.next()\n",
    "img_ary = np.array(images.astype('float'))\n",
    "rng = int(data.samples // 128)\n",
    "print(1)\n",
    "for _ in range(rng):\n",
    "    \n",
    "    images= data.next()\n",
    "    img_ary = np.vstack((img_ary, images.astype('float')))\n",
    "    \n",
    "print(2)    \n",
    "\n",
    "#channel_means = img_ary.mean(axis=(0,1,2)).astype('float')\n",
    "#img_ary = img_ary - channel_means\n",
    "    \n",
    "train_X_unlab = img_ary\n",
    "train_X_unlab=train_X_unlab/train_X_unlab.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_unlab=[]\n",
    "for name in data.filenames:\n",
    "    if \"\\p\" in name:\n",
    "        train_y_unlab.append(0)\n",
    "        \n",
    "    else:\n",
    "        train_y_unlab.append(1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_unlab=np.array(train_y_unlab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_unlab=train_y_unlab.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y_unlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9852"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_X_unlab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:346: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, which overrides setting of `featurewise_center`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    }
   ],
   "source": [
    "gen_normalize=ImageDataGenerator(featurewise_std_normalization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformation(data):\n",
    "    gen_normalize.fit(data)\n",
    "    train_iterator = gen_normalize.flow(data,batch_size=len(data))\n",
    "    data=train_iterator.next()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_X train_y train_X_val train_y_val  train_X_unlab train_y_unlab\n",
    "train_X_full=np.concatenate((train_X,train_X_unlab),axis=0)\n",
    "train_y_full=np.concatenate((train_y,train_y_unlab),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9972"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_X_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_X_full=transformation(train_X_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.4850262 ,  0.46697989,  0.48618089],\n",
       "         [ 0.4850262 ,  0.46697989,  0.48618089],\n",
       "         [ 0.4850262 ,  0.46697989,  0.48618089],\n",
       "         ...,\n",
       "         [ 0.45959539,  0.46697989,  0.40988847],\n",
       "         [ 0.45959539,  0.46697989,  0.40988847],\n",
       "         [ 0.45959539,  0.46697989,  0.40988847]],\n",
       "\n",
       "        [[ 0.4850262 ,  0.46697989,  0.48618089],\n",
       "         [ 0.4850262 ,  0.46697989,  0.48618089],\n",
       "         [ 0.4850262 ,  0.46697989,  0.48618089],\n",
       "         ...,\n",
       "         [ 0.45959539,  0.46697989,  0.40988847],\n",
       "         [ 0.45959539,  0.46697989,  0.40988847],\n",
       "         [ 0.45959539,  0.46697989,  0.40988847]],\n",
       "\n",
       "        [[ 0.45959539,  0.45426449,  0.46710778],\n",
       "         [ 0.45959539,  0.45426449,  0.46710778],\n",
       "         [ 0.45959539,  0.45426449,  0.46710778],\n",
       "         ...,\n",
       "         [ 0.36422987,  0.37161437,  0.31452295],\n",
       "         [ 0.36422987,  0.37161437,  0.31452295],\n",
       "         [ 0.36422987,  0.37161437,  0.31452295]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.34515677,  0.35254127,  0.46075008],\n",
       "         [ 0.34515677,  0.35254127,  0.46075008],\n",
       "         [ 0.34515677,  0.35254127,  0.46075008],\n",
       "         ...,\n",
       "         [-0.06173612,  0.28896425,  0.33359605],\n",
       "         [-0.06173612,  0.28896425,  0.33359605],\n",
       "         [-0.06173612,  0.28896425,  0.33359605]],\n",
       "\n",
       "        [[ 0.40237608,  0.40340288,  0.53068479],\n",
       "         [ 0.40237608,  0.40340288,  0.53068479],\n",
       "         [ 0.40237608,  0.40340288,  0.53068479],\n",
       "         ...,\n",
       "         [-0.03630531,  0.28896425,  0.30816525],\n",
       "         [-0.03630531,  0.28896425,  0.30816525],\n",
       "         [-0.03630531,  0.28896425,  0.30816525]],\n",
       "\n",
       "        [[ 0.40237608,  0.40340288,  0.53068479],\n",
       "         [ 0.40237608,  0.40340288,  0.53068479],\n",
       "         [ 0.40237608,  0.40340288,  0.53068479],\n",
       "         ...,\n",
       "         [-0.03630531,  0.28896425,  0.30816525],\n",
       "         [-0.03630531,  0.28896425,  0.30816525],\n",
       "         [-0.03630531,  0.28896425,  0.30816525]]],\n",
       "\n",
       "\n",
       "       [[[ 0.35151447,  0.33982586,  0.33995376],\n",
       "         [ 0.44052229,  0.42883368,  0.42896157],\n",
       "         [ 0.4977416 ,  0.48605299,  0.48618089],\n",
       "         ...,\n",
       "         [ 0.41509148,  0.47333759,  0.47982318],\n",
       "         [ 0.40237608,  0.47969529,  0.47982318],\n",
       "         [ 0.40237608,  0.47969529,  0.47982318]],\n",
       "\n",
       "        [[ 0.38330297,  0.37161437,  0.37174226],\n",
       "         [ 0.46595309,  0.45426449,  0.45439238],\n",
       "         [ 0.5040993 ,  0.49241069,  0.49253859],\n",
       "         ...,\n",
       "         [ 0.41509148,  0.47333759,  0.47982318],\n",
       "         [ 0.41509148,  0.47333759,  0.47982318],\n",
       "         [ 0.40237608,  0.47969529,  0.47982318]],\n",
       "\n",
       "        [[ 0.39601838,  0.39068747,  0.39081537],\n",
       "         [ 0.45959539,  0.45426449,  0.45439238],\n",
       "         [ 0.47231079,  0.46697989,  0.46710778],\n",
       "         ...,\n",
       "         [ 0.41509148,  0.47333759,  0.47982318],\n",
       "         [ 0.41509148,  0.47333759,  0.47982318],\n",
       "         [ 0.40237608,  0.47969529,  0.47982318]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.27154026, -0.25144036, -0.23223936],\n",
       "         [-0.27789796, -0.25779806, -0.23859706],\n",
       "         [-0.09352462, -0.07342472, -0.06693913],\n",
       "         ...,\n",
       "         [-0.37962118, -0.12428633,  0.72777354],\n",
       "         [-0.37326348, -0.13064403,  0.70870043],\n",
       "         [-0.34147497, -0.14971714,  0.60697721]],\n",
       "\n",
       "        [[ 0.28793746,  0.27624885,  0.25730364],\n",
       "         [ 0.19257194,  0.18088333,  0.16193812],\n",
       "         [ 0.17985653,  0.17452563,  0.13650731],\n",
       "         ...,\n",
       "         [-0.39233658, -0.13064403,  0.68326963],\n",
       "         [-0.38597888, -0.13064403,  0.70870043],\n",
       "         [-0.33511727, -0.10521323,  0.68326963]],\n",
       "\n",
       "        [[ 0.56767631,  0.5432723 ,  0.43531928],\n",
       "         [ 0.56767631,  0.5432723 ,  0.43531928],\n",
       "         [ 0.30701056,  0.28260655,  0.18736893],\n",
       "         ...,\n",
       "         [-0.39233658, -0.11792863,  0.60061951],\n",
       "         [-0.40505199, -0.11792863,  0.70234273],\n",
       "         [-0.37326348, -0.11792863,  0.73413124]]],\n",
       "\n",
       "\n",
       "       [[[ 0.32608366,  0.32075276,  0.34631146],\n",
       "         [ 0.35151447,  0.34618356,  0.37174226],\n",
       "         [ 0.30701056,  0.31439506,  0.34631146],\n",
       "         ...,\n",
       "         [ 0.56767631,  0.5241992 ,  0.33359605],\n",
       "         [ 0.56131861,  0.5178415 ,  0.31452295],\n",
       "         [ 0.55496091,  0.5241992 ,  0.30180755]],\n",
       "\n",
       "        [[ 0.17985653,  0.14909482,  0.18736893],\n",
       "         [ 0.20528734,  0.18724103,  0.21915743],\n",
       "         [ 0.21164504,  0.19995644,  0.23823054],\n",
       "         ...,\n",
       "         [ 0.56767631,  0.5241992 ,  0.33359605],\n",
       "         [ 0.56767631,  0.5241992 ,  0.32088065],\n",
       "         [ 0.56767631,  0.5369146 ,  0.31452295]],\n",
       "\n",
       "        [[ 0.31972596,  0.24446034,  0.28273444],\n",
       "         [ 0.26886435,  0.21267184,  0.25730364],\n",
       "         [ 0.27522205,  0.23174494,  0.27001904],\n",
       "         ...,\n",
       "         [ 0.57403401,  0.5305569 ,  0.32723835],\n",
       "         [ 0.55496091,  0.5241992 ,  0.31452295],\n",
       "         [ 0.56131861,  0.5305569 ,  0.30816525]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.59578302, -0.60111393, -0.65184765],\n",
       "         [-0.59578302, -0.60111393, -0.65184765],\n",
       "         [-0.57670992, -0.58204082, -0.63277454],\n",
       "         ...,\n",
       "         [ 0.44687999,  0.43519138,  0.18736893],\n",
       "         [ 0.43416458,  0.45426449,  0.19372663],\n",
       "         [ 0.44052229,  0.44154908,  0.18736893]],\n",
       "\n",
       "        [[-0.55127912, -0.55661002, -0.60734374],\n",
       "         [-0.55127912, -0.55661002, -0.60734374],\n",
       "         [-0.56399452, -0.56932542, -0.62005914],\n",
       "         ...,\n",
       "         [ 0.44052229,  0.42883368,  0.18101122],\n",
       "         [ 0.42780688,  0.44790679,  0.18736893],\n",
       "         [ 0.42144918,  0.44154908,  0.18101122]],\n",
       "\n",
       "        [[-0.62121383, -0.62654473, -0.67727845],\n",
       "         [-0.59578302, -0.60111393, -0.65184765],\n",
       "         [-0.59578302, -0.60111393, -0.65184765],\n",
       "         ...,\n",
       "         [ 0.44687999,  0.44154908,  0.17465352],\n",
       "         [ 0.42144918,  0.44154908,  0.16829582],\n",
       "         [ 0.41509148,  0.43519138,  0.16193812]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[-0.0941571 ,  0.07792077,  0.05770375],\n",
       "         [ 0.00126358,  0.17970283,  0.17220856],\n",
       "         [-0.03690469,  0.13517318,  0.1594858 ],\n",
       "         ...,\n",
       "         [ 0.41475318,  0.42779659,  0.21037683],\n",
       "         [ 0.43383731,  0.44688073,  0.22946097],\n",
       "         [ 0.44019869,  0.4532421 ,  0.23582235]],\n",
       "\n",
       "        [[-0.08143434,  0.01430699,  0.13404029],\n",
       "         [ 0.00762496,  0.10336629,  0.22309959],\n",
       "         [ 0.03307047,  0.14153456,  0.2485451 ],\n",
       "         ...,\n",
       "         [ 0.39566904,  0.40235108,  0.20401545],\n",
       "         [ 0.41475318,  0.42143521,  0.22309959],\n",
       "         [ 0.42111456,  0.42779659,  0.22946097]],\n",
       "\n",
       "        [[ 0.07760012,  0.13517318,  0.2485451 ],\n",
       "         [ 0.00126358,  0.06519802,  0.14676305],\n",
       "         [ 0.07123874,  0.12245042,  0.18493132],\n",
       "         ...,\n",
       "         [ 0.42747594,  0.41507383,  0.24218372],\n",
       "         [ 0.44656007,  0.43415797,  0.26126786],\n",
       "         [ 0.44019869,  0.42779659,  0.25490648]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.62215151, -0.6091081 , -0.58479548],\n",
       "         [-0.62215151, -0.61546948, -0.59115686],\n",
       "         [-0.56489911, -0.53277156, -0.50209756],\n",
       "         ...,\n",
       "         [-0.2722757 , -0.2846778 , -0.29853345],\n",
       "         [-0.26591432, -0.27195504, -0.2540038 ],\n",
       "         [-0.31044397, -0.29103918, -0.26672655]],\n",
       "\n",
       "        [[-0.28499845, -0.25287091, -0.22219691],\n",
       "         [-0.26591432, -0.22742539, -0.20947415],\n",
       "         [-0.19593916, -0.14472747, -0.13313761],\n",
       "         ...,\n",
       "         [-0.32316673, -0.34193021, -0.35578585],\n",
       "         [-0.33588948, -0.36101434, -0.34942448],\n",
       "         [-0.31044397, -0.31648469, -0.28581069]],\n",
       "\n",
       "        [[-0.20866191, -0.14472747, -0.05043969],\n",
       "         [-0.23410743, -0.18925712, -0.10133071],\n",
       "         [-0.20866191, -0.16381161, -0.08860796],\n",
       "         ...,\n",
       "         [-0.33588948, -0.34193021, -0.37486999],\n",
       "         [-0.34861224, -0.36737572, -0.39395412],\n",
       "         [-0.32316673, -0.36101434, -0.38123137]]],\n",
       "\n",
       "\n",
       "       [[[ 0.19846631,  0.28784626,  0.22309959],\n",
       "         [ 0.21755045,  0.3069304 ,  0.24218372],\n",
       "         [ 0.21118907,  0.28148489,  0.23582235],\n",
       "         ...,\n",
       "         [ 0.45928283,  0.42143521,  0.2485451 ],\n",
       "         [ 0.47836696,  0.44051935,  0.26762924],\n",
       "         [ 0.46564421,  0.42779659,  0.25490648]],\n",
       "\n",
       "        [[ 0.27480285,  0.34509867,  0.28671337],\n",
       "         [ 0.30024837,  0.37054418,  0.31215889],\n",
       "         [ 0.24935734,  0.3069304 ,  0.25490648],\n",
       "         ...,\n",
       "         [ 0.43383731,  0.3959897 ,  0.21037683],\n",
       "         [ 0.44019869,  0.40235108,  0.21673821],\n",
       "         [ 0.44656007,  0.40871246,  0.22309959]],\n",
       "\n",
       "        [[ 0.24935734,  0.3069304 ,  0.25490648],\n",
       "         [ 0.10940701,  0.16698007,  0.11495615],\n",
       "         [ 0.2620801 ,  0.31329178,  0.26126786],\n",
       "         ...,\n",
       "         [ 0.42111456,  0.40235108,  0.19765408],\n",
       "         [ 0.44656007,  0.42779659,  0.22309959],\n",
       "         [ 0.42111456,  0.40235108,  0.19765408]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.27863708, -0.24650953, -0.30489483],\n",
       "         [-0.25955294, -0.22742539, -0.28581069],\n",
       "         [-0.29135983, -0.26559366, -0.28581069],\n",
       "         ...,\n",
       "         [-0.12596399, -0.11928196, -0.29853345],\n",
       "         [-0.19593916, -0.1956185 , -0.35578585],\n",
       "         [-0.23410743, -0.23378677, -0.39395412]],\n",
       "\n",
       "        [[-0.06235021, -0.0493068 , -0.03771693],\n",
       "         [-0.04326607, -0.0174999 , -0.02499417],\n",
       "         [-0.13232537, -0.04294542, -0.12041485],\n",
       "         ...,\n",
       "         [-0.10051848, -0.10019782, -0.26036518],\n",
       "         [-0.13232537, -0.13200472, -0.27944931],\n",
       "         [-0.19593916, -0.1956185 , -0.3430631 ]],\n",
       "\n",
       "        [[-0.39950327, -0.33556883, -0.35578585],\n",
       "         [-0.55853773, -0.50096467, -0.54026583],\n",
       "         [-0.60306738, -0.59638535, -0.67385478],\n",
       "         ...,\n",
       "         [-0.11324124, -0.11292058, -0.26036518],\n",
       "         [-0.10687986, -0.1065592 , -0.2540038 ],\n",
       "         [-0.15140951, -0.15108885, -0.29853345]]],\n",
       "\n",
       "\n",
       "       [[[ 0.32569388,  0.36418281,  0.27399062],\n",
       "         [ 0.20482769,  0.24331661,  0.14676305],\n",
       "         [ 0.22391182,  0.24331661,  0.10859478],\n",
       "         ...,\n",
       "         [ 0.43383731,  0.42143521,  0.19765408],\n",
       "         [ 0.44019869,  0.42779659,  0.20401545],\n",
       "         [ 0.44019869,  0.42779659,  0.20401545]],\n",
       "\n",
       "        [[ 0.37022353,  0.40871246,  0.31852027],\n",
       "         [ 0.2620801 ,  0.30056902,  0.21037683],\n",
       "         [ 0.28116423,  0.30056902,  0.16584718],\n",
       "         ...,\n",
       "         [ 0.43383731,  0.42143521,  0.19765408],\n",
       "         [ 0.44019869,  0.42779659,  0.20401545],\n",
       "         [ 0.44019869,  0.42779659,  0.20401545]],\n",
       "\n",
       "        [[ 0.34477802,  0.3959897 ,  0.31852027],\n",
       "         [ 0.27480285,  0.31329178,  0.22309959],\n",
       "         [ 0.30024837,  0.31329178,  0.19765408],\n",
       "         ...,\n",
       "         [ 0.43383731,  0.42143521,  0.19765408],\n",
       "         [ 0.44019869,  0.42779659,  0.20401545],\n",
       "         [ 0.44019869,  0.42779659,  0.20401545]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.59034462, -0.53277156, -0.64840926],\n",
       "         [-0.62215151, -0.57093983, -0.6674934 ],\n",
       "         [-0.62215151, -0.62183086, -0.62932513],\n",
       "         ...,\n",
       "         [-0.0941571 , -0.10019782, -0.22219691],\n",
       "         [-0.08143434, -0.08111369, -0.21583553],\n",
       "         [-0.08779572, -0.08747507, -0.22219691]],\n",
       "\n",
       "        [[-0.62215151, -0.58366259, -0.64840926],\n",
       "         [-0.62215151, -0.6091081 , -0.66113202],\n",
       "         [-0.61579014, -0.59638535, -0.57207272],\n",
       "         ...,\n",
       "         [-0.10687986, -0.11292058, -0.23491966],\n",
       "         [-0.08143434, -0.08111369, -0.21583553],\n",
       "         [-0.08779572, -0.08747507, -0.22219691]],\n",
       "\n",
       "        [[-0.29772121, -0.27831642, -0.1649445 ],\n",
       "         [-0.62215151, -0.6091081 , -0.48301342],\n",
       "         [-0.596706  , -0.59638535, -0.44484515],\n",
       "         ...,\n",
       "         [-0.11324124, -0.11928196, -0.24128104],\n",
       "         [-0.11324124, -0.11292058, -0.26036518],\n",
       "         [-0.11960261, -0.11928196, -0.26672655]]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_X_val=transformation(train_X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1254880af60>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_X_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobile_net_base = VGG16(weights='imagenet', include_top=False, input_shape = (128,128,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x12b20889630>,\n",
       " <keras.layers.convolutional.Conv2D at 0x12b20889f60>,\n",
       " <keras.layers.convolutional.Conv2D at 0x12b20889f98>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x12b208dae10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x12b208da828>,\n",
       " <keras.layers.convolutional.Conv2D at 0x12b209086d8>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x12b2093ba58>,\n",
       " <keras.layers.convolutional.Conv2D at 0x12b2093bc18>,\n",
       " <keras.layers.convolutional.Conv2D at 0x12b2094dc88>,\n",
       " <keras.layers.convolutional.Conv2D at 0x12b20980d30>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x12b209a0cc0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x12b209a0dd8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x12b209d16a0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x12b209f0e10>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x12b20a23ef0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x12b20a23668>,\n",
       " <keras.layers.convolutional.Conv2D at 0x12b20a56ac8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x12b20a70a58>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x12b20aa5ac8>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mobile_net_base.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x12b20889630>,\n",
       " <keras.layers.convolutional.Conv2D at 0x12b20889f60>,\n",
       " <keras.layers.convolutional.Conv2D at 0x12b20889f98>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x12b208dae10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x12b208da828>,\n",
       " <keras.layers.convolutional.Conv2D at 0x12b209086d8>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x12b2093ba58>,\n",
       " <keras.layers.convolutional.Conv2D at 0x12b2093bc18>,\n",
       " <keras.layers.convolutional.Conv2D at 0x12b2094dc88>,\n",
       " <keras.layers.convolutional.Conv2D at 0x12b20980d30>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x12b209a0cc0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x12b209a0dd8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x12b209d16a0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x12b209f0e10>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x12b20a23ef0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x12b20a23668>,\n",
       " <keras.layers.convolutional.Conv2D at 0x12b20a56ac8>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mobile_net_base.layers[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_design():\n",
    "    vgg16_base = VGG16(weights='imagenet', include_top=False, input_shape = (128,128,3))\n",
    "    vgg16_base.layers\n",
    "    for layer in vgg16_base.layers[:-2]:\n",
    "        layer.trainable=False\n",
    "    x=vgg16_base.output\n",
    "    x=Dropout(0.10)(x)\n",
    "    x= Flatten()(x)\n",
    "    x=Dropout(0.20)(x)\n",
    "    x=Dense(512,activation='relu')(x)\n",
    "    x=Dropout(0.30)(x)\n",
    "    \n",
    "    x=Dense(2,activation='softmax')(x)\n",
    "  \n",
    "    return x,vgg16_base.input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,inps=model_design()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Model(inputs=inps,outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(Adam(lr=1e-6), loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folds = []\n",
    "k = 3\n",
    "def load_data_kfold(k, train_X, train_y1):\n",
    "    print(\"Enter Load_data_kfold\")\n",
    "    global folds\n",
    "    folds = list(StratifiedKFold(n_splits=k, shuffle=True, random_state=1).split(train_X, train_y1))\n",
    "    print(\"Exit Load_data_kfold\")\n",
    "    return folds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Load_data_kfold\n",
      "Exit Load_data_kfold\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(array([   0,    1,    2, ..., 9968, 9969, 9970]),\n",
       "  array([   5,   11,   12,   19,   21,   37,   57,   73,   78,   80,   92,\n",
       "           98,  102,  103,  107,  112,  122,  139,  158,  168,  186,  221,\n",
       "          225,  229,  241,  252,  260,  263,  266,  279,  282,  304,  308,\n",
       "          335,  336,  340,  345,  359,  374,  382,  385,  408,  409,  419,\n",
       "          432,  454,  488,  500,  507,  517,  561,  563,  576,  577,  604,\n",
       "          612,  615,  616,  626,  634,  652,  660,  668,  670,  683,  696,\n",
       "          704,  705,  738,  744,  747,  778,  784,  789,  819,  821,  831,\n",
       "          839,  843,  855,  856,  859,  864,  896,  899,  914,  918,  923,\n",
       "          930,  931,  934,  941,  951,  955,  959,  961,  966,  970,  991,\n",
       "         1001, 1018, 1047, 1056, 1071, 1090, 1094, 1098, 1101, 1102, 1104,\n",
       "         1111, 1120, 1141, 1154, 1181, 1186, 1200, 1201, 1215, 1220, 1225,\n",
       "         1229, 1245, 1295, 1296, 1302, 1306, 1307, 1316, 1331, 1337, 1357,\n",
       "         1412, 1415, 1437, 1440, 1441, 1446, 1447, 1452, 1455, 1458, 1459,\n",
       "         1470, 1472, 1473, 1474, 1477, 1480, 1486, 1492, 1517, 1523, 1536,\n",
       "         1537, 1554, 1557, 1573, 1580, 1586, 1597, 1606, 1610, 1619, 1620,\n",
       "         1621, 1624, 1637, 1639, 1644, 1668, 1675, 1680, 1697, 1712, 1713,\n",
       "         1717, 1770, 1787, 1789, 1796, 1800, 1802, 1805, 1837, 1851, 1870,\n",
       "         1893, 1908, 1917, 1952, 1957, 1975, 1976, 1982, 1984, 1989, 2001,\n",
       "         2007, 2022, 2023, 2037, 2046, 2059, 2060, 2071, 2075, 2097, 2098,\n",
       "         2108, 2111, 2124, 2125, 2130, 2133, 2138, 2144, 2166, 2177, 2179,\n",
       "         2180, 2206, 2207, 2215, 2234, 2239, 2257, 2261, 2273, 2283, 2309,\n",
       "         2322, 2328, 2354, 2360, 2378, 2379, 2380, 2387, 2390, 2394, 2398,\n",
       "         2421, 2425, 2442, 2443, 2449, 2462, 2476, 2481, 2484, 2501, 2503,\n",
       "         2513, 2516, 2536, 2574, 2580, 2583, 2596, 2598, 2601, 2620, 2627,\n",
       "         2633, 2636, 2642, 2655, 2657, 2660, 2666, 2672, 2681, 2687, 2696,\n",
       "         2730, 2733, 2744, 2746, 2758, 2761, 2763, 2767, 2780, 2781, 2782,\n",
       "         2783, 2788, 2798, 2799, 2801, 2803, 2810, 2814, 2832, 2854, 2863,\n",
       "         2879, 2906, 2914, 2938, 2943, 2969, 2988, 2994, 3002, 3009, 3011,\n",
       "         3028, 3029, 3041, 3042, 3056, 3075, 3080, 3083, 3086, 3093, 3102,\n",
       "         3108, 3112, 3120, 3131, 3133, 3138, 3144, 3179, 3180, 3185, 3192,\n",
       "         3193, 3220, 3222, 3227, 3246, 3254, 3255, 3270, 3286, 3288, 3292,\n",
       "         3299, 3300, 3319, 3329, 3332, 3341, 3367, 3379, 3381, 3383, 3388,\n",
       "         3396, 3439, 3457, 3458, 3473, 3476, 3505, 3524, 3536, 3572, 3587,\n",
       "         3592, 3596, 3612, 3624, 3625, 3641, 3654, 3701, 3709, 3712, 3723,\n",
       "         3727, 3735, 3745, 3750, 3756, 3767, 3776, 3781, 3791, 3803, 3809,\n",
       "         3816, 3829, 3842, 3844, 3849, 3873, 3877, 3894, 3918, 3922, 3927,\n",
       "         3940, 3964, 3980, 3994, 4026, 4043, 4050, 4052, 4053, 4061, 4062,\n",
       "         4076, 4078, 4079, 4100, 4109, 4113, 4118, 4127, 4133, 4140, 4141,\n",
       "         4142, 4147, 4149, 4170, 4177, 4184, 4185, 4193, 4197, 4199, 4227,\n",
       "         4236, 4239, 4267, 4289, 4294, 4310, 4325, 4334, 4338, 4348, 4350,\n",
       "         4351, 4360, 4361, 4363, 4366, 4371, 4373, 4390, 4409, 4412, 4424,\n",
       "         4426, 4434, 4443, 4448, 4467, 4485, 4495, 4496, 4506, 4510, 4516,\n",
       "         4525, 4528, 4529, 4543, 4548, 4592, 4604, 4624, 4626, 4629, 4631,\n",
       "         4642, 4643, 4660, 4682, 4689, 4692, 4721, 4731, 4735, 4765, 4786,\n",
       "         4787, 4790, 4796, 4797, 4822, 4823, 4831, 4845, 4862, 4880, 4887,\n",
       "         4905, 4911, 4912, 4914, 4935, 4950, 4975, 4981, 4982, 5010, 5017,\n",
       "         5032, 5046, 5052, 5069, 5077, 5088, 5096, 5101, 5113, 5117, 5119,\n",
       "         5124, 5144, 5179, 5207, 5211, 5212, 5218, 5223, 5232, 5243, 5249,\n",
       "         5250, 5254, 5263, 5267, 5275, 5283, 5296, 5299, 5305, 5322, 5327,\n",
       "         5331, 5344, 5352, 5361, 5383, 5386, 5395, 5397, 5412, 5419, 5425,\n",
       "         5462, 5464, 5465, 5473, 5510, 5511, 5514, 5521, 5538, 5539, 5556,\n",
       "         5563, 5576, 5578, 5583, 5589, 5608, 5614, 5617, 5619, 5648, 5652,\n",
       "         5665, 5674, 5680, 5701, 5725, 5731, 5735, 5752, 5760, 5770, 5772,\n",
       "         5782, 5786, 5793, 5814, 5818, 5819, 5823, 5827, 5844, 5847, 5865,\n",
       "         5884, 5910, 5915, 5930, 5934, 5936, 5943, 5945, 5975, 5983, 6011,\n",
       "         6062, 6074, 6083, 6093, 6099, 6101, 6103, 6106, 6118, 6130, 6139,\n",
       "         6152, 6153, 6154, 6161, 6172, 6182, 6184, 6187, 6198, 6218, 6228,\n",
       "         6249, 6251, 6253, 6258, 6273, 6277, 6286, 6297, 6314, 6328, 6335,\n",
       "         6361, 6396, 6419, 6429, 6435, 6445, 6451, 6459, 6477, 6496, 6497,\n",
       "         6516, 6530, 6534, 6536, 6541, 6556, 6585, 6597, 6602, 6604, 6632,\n",
       "         6641, 6707, 6731, 6741, 6752, 6758, 6768, 6781, 6786, 6804, 6809,\n",
       "         6811, 6813, 6823, 6829, 6831, 6852, 6862, 6868, 6879, 6882, 6889,\n",
       "         6899, 6900, 6901, 6928, 6948, 6950, 6956, 6968, 6978, 6979, 6984,\n",
       "         6996, 7005, 7012, 7043, 7048, 7051, 7054, 7069, 7073, 7088, 7110,\n",
       "         7115, 7118, 7129, 7152, 7154, 7156, 7202, 7223, 7224, 7225, 7226,\n",
       "         7233, 7251, 7255, 7256, 7265, 7274, 7282, 7306, 7322, 7367, 7386,\n",
       "         7389, 7417, 7430, 7441, 7445, 7447, 7452, 7453, 7465, 7466, 7475,\n",
       "         7476, 7481, 7488, 7498, 7519, 7537, 7545, 7551, 7556, 7557, 7560,\n",
       "         7562, 7567, 7577, 7602, 7604, 7607, 7608, 7621, 7638, 7641, 7642,\n",
       "         7643, 7645, 7659, 7661, 7670, 7698, 7714, 7719, 7729, 7734, 7738,\n",
       "         7747, 7754, 7761, 7767, 7769, 7771, 7800, 7801, 7802, 7835, 7849,\n",
       "         7865, 7866, 7868, 7885, 7890, 7896, 7901, 7906, 7920, 7930, 7935,\n",
       "         7936, 7944, 7958, 7960, 7962, 7970, 7971, 7973, 7988, 7994, 8003,\n",
       "         8005, 8017, 8018, 8034, 8043, 8048, 8074, 8081, 8085, 8090, 8101,\n",
       "         8122, 8141, 8143, 8158, 8161, 8163, 8164, 8165, 8166, 8175, 8180,\n",
       "         8181, 8199, 8203, 8216, 8218, 8242, 8250, 8251, 8302, 8313, 8327,\n",
       "         8342, 8350, 8351, 8357, 8379, 8380, 8389, 8394, 8426, 8441, 8449,\n",
       "         8464, 8465, 8468, 8474, 8478, 8506, 8509, 8512, 8514, 8530, 8536,\n",
       "         8538, 8540, 8545, 8548, 8564, 8565, 8581, 8584, 8586, 8591, 8603,\n",
       "         8614, 8617, 8626, 8634, 8659, 8676, 8679, 8705, 8714, 8728, 8743,\n",
       "         8748, 8749, 8750, 8753, 8756, 8768, 8772, 8773, 8779, 8783, 8784,\n",
       "         8786, 8802, 8809, 8824, 8825, 8829, 8838, 8839, 8869, 8876, 8906,\n",
       "         8926, 8973, 8979, 8981, 8988, 8993, 9005, 9008, 9015, 9019, 9050,\n",
       "         9058, 9059, 9074, 9095, 9101, 9103, 9114, 9115, 9122, 9127, 9129,\n",
       "         9132, 9135, 9151, 9169, 9170, 9215, 9220, 9232, 9245, 9247, 9256,\n",
       "         9261, 9266, 9269, 9274, 9277, 9317, 9325, 9341, 9359, 9360, 9390,\n",
       "         9402, 9405, 9450, 9471, 9477, 9482, 9485, 9486, 9495, 9508, 9509,\n",
       "         9574, 9575, 9582, 9586, 9594, 9597, 9599, 9607, 9623, 9652, 9666,\n",
       "         9672, 9683, 9684, 9693, 9700, 9726, 9730, 9746, 9752, 9766, 9806,\n",
       "         9807, 9808, 9813, 9818, 9821, 9824, 9858, 9868, 9869, 9909, 9916,\n",
       "         9917, 9919, 9924, 9926, 9947, 9958, 9963, 9971])),\n",
       " (array([   0,    1,    2, ..., 9968, 9970, 9971]),\n",
       "  array([  13,   17,   28,   32,   40,   44,   45,   55,   58,   66,   67,\n",
       "           68,   72,   88,   95,   97,  100,  101,  105,  116,  142,  146,\n",
       "          150,  154,  159,  160,  162,  167,  171,  175,  179,  180,  183,\n",
       "          188,  193,  194,  196,  201,  217,  247,  248,  250,  251,  256,\n",
       "          278,  281,  290,  306,  310,  316,  333,  361,  364,  366,  373,\n",
       "          376,  384,  389,  390,  391,  395,  403,  404,  418,  424,  426,\n",
       "          453,  467,  513,  546,  554,  613,  617,  620,  633,  639,  642,\n",
       "          647,  649,  659,  703,  706,  722,  745,  754,  760,  763,  777,\n",
       "          805,  808,  809,  816,  817,  818,  823,  830,  832,  838,  851,\n",
       "          852,  868,  883,  894,  900,  945,  958,  981,  987,  992,  993,\n",
       "         1003, 1017, 1025, 1048, 1054, 1058, 1064, 1070, 1076, 1081, 1083,\n",
       "         1086, 1105, 1116, 1124, 1133, 1138, 1158, 1162, 1173, 1177, 1182,\n",
       "         1183, 1185, 1192, 1195, 1216, 1222, 1237, 1266, 1277, 1290, 1292,\n",
       "         1293, 1324, 1332, 1333, 1351, 1359, 1361, 1381, 1388, 1393, 1402,\n",
       "         1424, 1439, 1484, 1495, 1513, 1555, 1559, 1568, 1578, 1579, 1581,\n",
       "         1594, 1595, 1600, 1602, 1605, 1608, 1625, 1633, 1656, 1660, 1663,\n",
       "         1686, 1726, 1729, 1738, 1743, 1745, 1746, 1747, 1757, 1762, 1763,\n",
       "         1780, 1784, 1803, 1808, 1818, 1819, 1828, 1833, 1845, 1847, 1862,\n",
       "         1874, 1875, 1881, 1883, 1885, 1887, 1897, 1900, 1907, 1910, 1911,\n",
       "         1912, 1914, 1916, 1925, 1935, 1945, 1951, 1956, 1960, 1966, 1974,\n",
       "         1979, 1993, 1997, 2013, 2016, 2018, 2033, 2048, 2049, 2051, 2052,\n",
       "         2053, 2067, 2068, 2073, 2136, 2150, 2157, 2158, 2173, 2178, 2192,\n",
       "         2196, 2199, 2202, 2216, 2221, 2225, 2245, 2264, 2287, 2289, 2294,\n",
       "         2301, 2307, 2323, 2334, 2335, 2353, 2363, 2373, 2376, 2396, 2406,\n",
       "         2474, 2504, 2523, 2534, 2541, 2558, 2565, 2589, 2625, 2641, 2646,\n",
       "         2648, 2665, 2669, 2670, 2675, 2677, 2684, 2697, 2713, 2728, 2739,\n",
       "         2749, 2752, 2777, 2793, 2795, 2807, 2816, 2821, 2824, 2828, 2829,\n",
       "         2833, 2841, 2844, 2845, 2865, 2893, 2897, 2898, 2915, 2918, 2929,\n",
       "         2956, 2965, 2968, 2974, 2976, 2981, 2997, 3000, 3010, 3024, 3033,\n",
       "         3037, 3047, 3076, 3082, 3087, 3103, 3117, 3119, 3127, 3128, 3139,\n",
       "         3151, 3157, 3176, 3183, 3223, 3225, 3243, 3245, 3251, 3256, 3258,\n",
       "         3262, 3285, 3290, 3313, 3322, 3323, 3347, 3355, 3376, 3389, 3407,\n",
       "         3415, 3418, 3422, 3435, 3441, 3445, 3463, 3475, 3516, 3525, 3526,\n",
       "         3532, 3535, 3552, 3570, 3571, 3573, 3597, 3604, 3611, 3635, 3639,\n",
       "         3643, 3661, 3670, 3677, 3679, 3680, 3696, 3718, 3729, 3732, 3737,\n",
       "         3758, 3759, 3760, 3761, 3768, 3770, 3782, 3792, 3796, 3798, 3806,\n",
       "         3832, 3834, 3843, 3847, 3854, 3861, 3867, 3872, 3883, 3886, 3895,\n",
       "         3908, 3933, 3951, 3969, 3973, 3979, 3983, 3991, 3998, 4009, 4021,\n",
       "         4030, 4091, 4098, 4103, 4111, 4138, 4139, 4155, 4159, 4167, 4168,\n",
       "         4169, 4172, 4173, 4225, 4232, 4259, 4277, 4305, 4311, 4328, 4341,\n",
       "         4343, 4377, 4379, 4436, 4437, 4438, 4447, 4450, 4465, 4474, 4476,\n",
       "         4483, 4491, 4505, 4509, 4558, 4568, 4576, 4581, 4596, 4601, 4603,\n",
       "         4634, 4638, 4654, 4673, 4683, 4684, 4704, 4707, 4714, 4729, 4736,\n",
       "         4742, 4749, 4750, 4751, 4757, 4782, 4788, 4809, 4813, 4814, 4818,\n",
       "         4830, 4858, 4860, 4867, 4872, 4876, 4877, 4883, 4892, 4894, 4895,\n",
       "         4896, 4899, 4929, 4931, 4953, 4964, 4968, 4974, 4986, 5015, 5016,\n",
       "         5019, 5024, 5027, 5045, 5050, 5056, 5068, 5071, 5081, 5084, 5087,\n",
       "         5097, 5125, 5135, 5136, 5148, 5150, 5188, 5198, 5204, 5209, 5213,\n",
       "         5225, 5242, 5248, 5277, 5297, 5298, 5313, 5319, 5325, 5326, 5358,\n",
       "         5380, 5417, 5426, 5438, 5446, 5453, 5457, 5498, 5515, 5525, 5541,\n",
       "         5551, 5557, 5564, 5570, 5571, 5586, 5605, 5628, 5637, 5638, 5651,\n",
       "         5670, 5678, 5684, 5714, 5717, 5727, 5738, 5753, 5795, 5801, 5806,\n",
       "         5809, 5834, 5840, 5845, 5848, 5853, 5857, 5870, 5880, 5893, 5900,\n",
       "         5902, 5905, 5909, 5912, 5918, 5927, 5928, 5935, 5939, 5950, 5953,\n",
       "         5964, 5970, 5977, 5994, 5999, 6000, 6004, 6007, 6024, 6027, 6049,\n",
       "         6059, 6067, 6069, 6077, 6080, 6081, 6088, 6125, 6148, 6159, 6174,\n",
       "         6175, 6183, 6195, 6232, 6241, 6248, 6262, 6268, 6278, 6280, 6282,\n",
       "         6289, 6303, 6304, 6348, 6350, 6354, 6357, 6364, 6369, 6377, 6382,\n",
       "         6385, 6393, 6399, 6413, 6422, 6423, 6424, 6430, 6432, 6438, 6441,\n",
       "         6452, 6494, 6502, 6504, 6507, 6510, 6519, 6524, 6554, 6560, 6564,\n",
       "         6569, 6574, 6578, 6582, 6584, 6589, 6590, 6609, 6618, 6622, 6630,\n",
       "         6636, 6682, 6720, 6721, 6744, 6746, 6753, 6767, 6771, 6784, 6799,\n",
       "         6803, 6827, 6857, 6872, 6881, 6893, 6913, 6919, 6923, 6924, 6925,\n",
       "         6936, 6947, 6953, 6969, 6973, 6975, 6976, 6977, 7013, 7024, 7040,\n",
       "         7063, 7070, 7075, 7076, 7082, 7087, 7089, 7090, 7094, 7100, 7112,\n",
       "         7114, 7123, 7127, 7128, 7178, 7185, 7198, 7199, 7203, 7206, 7232,\n",
       "         7262, 7283, 7288, 7299, 7313, 7325, 7326, 7332, 7344, 7354, 7356,\n",
       "         7357, 7365, 7372, 7381, 7385, 7393, 7411, 7425, 7469, 7497, 7501,\n",
       "         7507, 7513, 7515, 7525, 7532, 7534, 7547, 7549, 7595, 7606, 7610,\n",
       "         7613, 7614, 7618, 7628, 7649, 7650, 7665, 7667, 7669, 7677, 7683,\n",
       "         7686, 7708, 7726, 7741, 7762, 7763, 7776, 7779, 7780, 7782, 7796,\n",
       "         7803, 7813, 7827, 7833, 7842, 7845, 7851, 7853, 7858, 7870, 7881,\n",
       "         7888, 7922, 7942, 7947, 7953, 7955, 7974, 8011, 8046, 8050, 8053,\n",
       "         8054, 8062, 8102, 8110, 8111, 8139, 8172, 8191, 8202, 8217, 8233,\n",
       "         8249, 8291, 8292, 8296, 8304, 8305, 8318, 8329, 8340, 8369, 8371,\n",
       "         8372, 8386, 8390, 8392, 8393, 8395, 8398, 8408, 8416, 8427, 8453,\n",
       "         8477, 8485, 8486, 8489, 8501, 8504, 8533, 8543, 8555, 8559, 8566,\n",
       "         8571, 8620, 8622, 8632, 8646, 8653, 8663, 8667, 8669, 8671, 8684,\n",
       "         8685, 8690, 8709, 8747, 8781, 8797, 8807, 8821, 8822, 8828, 8831,\n",
       "         8832, 8833, 8835, 8858, 8860, 8874, 8877, 8895, 8899, 8910, 8931,\n",
       "         8935, 8969, 8972, 8977, 8980, 8982, 8989, 8994, 9004, 9009, 9016,\n",
       "         9025, 9032, 9039, 9041, 9063, 9073, 9086, 9088, 9096, 9097, 9098,\n",
       "         9158, 9164, 9167, 9168, 9177, 9182, 9198, 9207, 9213, 9216, 9234,\n",
       "         9236, 9255, 9263, 9275, 9278, 9303, 9305, 9314, 9324, 9326, 9328,\n",
       "         9332, 9354, 9355, 9356, 9363, 9367, 9398, 9421, 9424, 9431, 9437,\n",
       "         9442, 9452, 9463, 9467, 9468, 9484, 9487, 9490, 9496, 9497, 9498,\n",
       "         9503, 9525, 9536, 9544, 9549, 9556, 9557, 9560, 9567, 9578, 9580,\n",
       "         9583, 9585, 9603, 9614, 9619, 9627, 9663, 9673, 9675, 9676, 9686,\n",
       "         9699, 9703, 9706, 9713, 9718, 9723, 9725, 9736, 9745, 9761, 9768,\n",
       "         9799, 9809, 9817, 9819, 9822, 9826, 9828, 9833, 9851, 9876, 9898,\n",
       "         9931, 9932, 9938, 9939, 9948, 9952, 9957, 9969])),\n",
       " (array([   0,    2,    3, ..., 9968, 9969, 9971]),\n",
       "  array([   1,    6,   26,   54,   75,   84,   91,  104,  117,  153,  161,\n",
       "          185,  190,  237,  246,  254,  261,  265,  270,  276,  284,  291,\n",
       "          292,  293,  296,  315,  320,  328,  350,  352,  362,  398,  402,\n",
       "          405,  411,  422,  428,  436,  445,  449,  452,  456,  458,  459,\n",
       "          476,  478,  480,  481,  495,  496,  498,  503,  508,  516,  520,\n",
       "          524,  541,  547,  550,  555,  578,  584,  585,  594,  608,  621,\n",
       "          623,  628,  637,  665,  667,  673,  676,  692,  710,  717,  721,\n",
       "          731,  755,  769,  770,  774,  779,  785,  790,  829,  833,  844,\n",
       "          845,  847,  848,  854,  857,  873,  889,  906,  917,  940,  953,\n",
       "          969,  973,  998, 1000, 1006, 1013, 1015, 1019, 1020, 1023, 1050,\n",
       "         1053, 1068, 1072, 1080, 1095, 1103, 1109, 1110, 1115, 1121, 1123,\n",
       "         1140, 1160, 1163, 1165, 1188, 1203, 1205, 1207, 1219, 1223, 1239,\n",
       "         1255, 1273, 1276, 1285, 1287, 1308, 1309, 1313, 1328, 1356, 1394,\n",
       "         1404, 1411, 1416, 1421, 1433, 1435, 1445, 1461, 1463, 1464, 1466,\n",
       "         1479, 1482, 1487, 1493, 1498, 1501, 1504, 1508, 1516, 1518, 1558,\n",
       "         1572, 1583, 1584, 1585, 1588, 1593, 1609, 1612, 1614, 1631, 1638,\n",
       "         1646, 1673, 1681, 1685, 1695, 1720, 1732, 1733, 1735, 1736, 1774,\n",
       "         1777, 1778, 1781, 1783, 1794, 1810, 1811, 1832, 1843, 1844, 1852,\n",
       "         1867, 1882, 1888, 1892, 1896, 1919, 1959, 1969, 1973, 1986, 1988,\n",
       "         1998, 1999, 2006, 2019, 2021, 2047, 2076, 2077, 2084, 2085, 2087,\n",
       "         2090, 2099, 2100, 2103, 2106, 2114, 2128, 2183, 2193, 2252, 2253,\n",
       "         2297, 2303, 2305, 2316, 2317, 2330, 2339, 2405, 2407, 2418, 2420,\n",
       "         2422, 2426, 2437, 2438, 2448, 2450, 2454, 2478, 2483, 2502, 2517,\n",
       "         2519, 2530, 2531, 2533, 2540, 2547, 2573, 2577, 2595, 2597, 2603,\n",
       "         2615, 2629, 2643, 2654, 2663, 2668, 2678, 2695, 2704, 2712, 2725,\n",
       "         2732, 2735, 2737, 2738, 2742, 2754, 2755, 2787, 2791, 2802, 2808,\n",
       "         2822, 2839, 2860, 2872, 2884, 2891, 2896, 2899, 2916, 2921, 2923,\n",
       "         2932, 2933, 2941, 2963, 2966, 3012, 3026, 3031, 3044, 3053, 3059,\n",
       "         3134, 3137, 3156, 3164, 3165, 3166, 3187, 3204, 3215, 3219, 3236,\n",
       "         3249, 3252, 3266, 3302, 3304, 3308, 3312, 3342, 3345, 3346, 3357,\n",
       "         3365, 3370, 3377, 3378, 3384, 3390, 3391, 3394, 3400, 3450, 3452,\n",
       "         3453, 3464, 3480, 3482, 3486, 3492, 3497, 3504, 3507, 3510, 3511,\n",
       "         3515, 3559, 3565, 3582, 3585, 3591, 3600, 3606, 3613, 3628, 3642,\n",
       "         3644, 3664, 3668, 3672, 3688, 3690, 3698, 3702, 3720, 3738, 3746,\n",
       "         3775, 3780, 3807, 3812, 3813, 3819, 3825, 3831, 3837, 3840, 3863,\n",
       "         3864, 3876, 3889, 3907, 3925, 3938, 3944, 3954, 3959, 3976, 3982,\n",
       "         3986, 3987, 4004, 4035, 4036, 4039, 4045, 4048, 4054, 4068, 4083,\n",
       "         4086, 4090, 4092, 4093, 4101, 4115, 4128, 4132, 4148, 4152, 4153,\n",
       "         4154, 4158, 4163, 4190, 4200, 4234, 4235, 4245, 4254, 4257, 4263,\n",
       "         4280, 4296, 4316, 4329, 4339, 4345, 4354, 4359, 4369, 4372, 4376,\n",
       "         4382, 4414, 4415, 4432, 4435, 4441, 4442, 4452, 4453, 4454, 4462,\n",
       "         4487, 4502, 4508, 4511, 4537, 4538, 4540, 4544, 4556, 4573, 4577,\n",
       "         4580, 4583, 4584, 4612, 4613, 4616, 4620, 4639, 4650, 4659, 4681,\n",
       "         4706, 4717, 4719, 4724, 4741, 4745, 4754, 4756, 4783, 4802, 4805,\n",
       "         4806, 4811, 4828, 4829, 4834, 4865, 4873, 4913, 4922, 4932, 4936,\n",
       "         4939, 4955, 4962, 4967, 4971, 4988, 5036, 5039, 5042, 5054, 5064,\n",
       "         5072, 5078, 5080, 5092, 5095, 5118, 5127, 5139, 5140, 5156, 5162,\n",
       "         5170, 5172, 5176, 5181, 5186, 5197, 5200, 5205, 5210, 5224, 5229,\n",
       "         5233, 5235, 5247, 5256, 5258, 5265, 5269, 5282, 5292, 5300, 5304,\n",
       "         5316, 5320, 5324, 5332, 5348, 5364, 5368, 5374, 5375, 5387, 5393,\n",
       "         5394, 5407, 5409, 5410, 5413, 5428, 5433, 5455, 5470, 5472, 5488,\n",
       "         5496, 5505, 5507, 5534, 5552, 5572, 5573, 5584, 5594, 5602, 5606,\n",
       "         5612, 5616, 5622, 5631, 5633, 5639, 5673, 5676, 5686, 5723, 5730,\n",
       "         5733, 5749, 5759, 5811, 5816, 5830, 5852, 5856, 5861, 5867, 5871,\n",
       "         5876, 5877, 5886, 5896, 5911, 5942, 5948, 5965, 5987, 6013, 6023,\n",
       "         6038, 6041, 6052, 6065, 6092, 6096, 6111, 6114, 6115, 6128, 6129,\n",
       "         6133, 6134, 6137, 6140, 6145, 6147, 6156, 6166, 6193, 6203, 6206,\n",
       "         6209, 6214, 6222, 6223, 6225, 6231, 6255, 6256, 6263, 6272, 6283,\n",
       "         6288, 6301, 6309, 6310, 6321, 6327, 6347, 6386, 6389, 6395, 6421,\n",
       "         6437, 6447, 6464, 6469, 6476, 6478, 6480, 6482, 6499, 6509, 6526,\n",
       "         6529, 6533, 6535, 6544, 6546, 6549, 6551, 6561, 6565, 6572, 6580,\n",
       "         6583, 6601, 6607, 6617, 6621, 6633, 6650, 6657, 6659, 6667, 6674,\n",
       "         6711, 6712, 6724, 6733, 6739, 6745, 6748, 6761, 6766, 6770, 6779,\n",
       "         6787, 6793, 6805, 6835, 6886, 6887, 6890, 6904, 6929, 6934, 6955,\n",
       "         6958, 6959, 6962, 6974, 6983, 7001, 7009, 7017, 7019, 7030, 7033,\n",
       "         7035, 7037, 7041, 7042, 7049, 7050, 7053, 7067, 7079, 7101, 7105,\n",
       "         7111, 7117, 7148, 7160, 7175, 7182, 7188, 7190, 7191, 7194, 7195,\n",
       "         7216, 7227, 7234, 7236, 7247, 7248, 7252, 7269, 7271, 7276, 7287,\n",
       "         7290, 7296, 7297, 7300, 7312, 7317, 7319, 7320, 7335, 7349, 7362,\n",
       "         7380, 7383, 7388, 7405, 7419, 7427, 7432, 7449, 7460, 7477, 7478,\n",
       "         7496, 7506, 7516, 7542, 7548, 7571, 7575, 7580, 7587, 7593, 7599,\n",
       "         7605, 7609, 7620, 7630, 7631, 7646, 7651, 7654, 7655, 7671, 7679,\n",
       "         7690, 7728, 7730, 7746, 7753, 7758, 7764, 7773, 7775, 7815, 7852,\n",
       "         7855, 7861, 7871, 7872, 7880, 7883, 7889, 7899, 7904, 7919, 7923,\n",
       "         7926, 7929, 7943, 7964, 7978, 7993, 7995, 8007, 8008, 8014, 8015,\n",
       "         8027, 8030, 8051, 8077, 8082, 8094, 8099, 8105, 8107, 8109, 8114,\n",
       "         8115, 8186, 8193, 8196, 8211, 8224, 8243, 8246, 8257, 8259, 8267,\n",
       "         8293, 8312, 8320, 8326, 8361, 8373, 8376, 8381, 8384, 8412, 8420,\n",
       "         8422, 8428, 8437, 8450, 8461, 8462, 8471, 8473, 8492, 8505, 8508,\n",
       "         8519, 8520, 8526, 8535, 8537, 8541, 8546, 8550, 8553, 8573, 8574,\n",
       "         8576, 8578, 8580, 8589, 8594, 8616, 8631, 8664, 8678, 8718, 8726,\n",
       "         8731, 8732, 8740, 8776, 8798, 8799, 8801, 8834, 8855, 8873, 8888,\n",
       "         8919, 8925, 8942, 8943, 8986, 8997, 8999, 9047, 9091, 9110, 9113,\n",
       "         9116, 9141, 9144, 9160, 9161, 9181, 9183, 9188, 9193, 9194, 9208,\n",
       "         9210, 9229, 9233, 9240, 9243, 9253, 9259, 9264, 9295, 9297, 9315,\n",
       "         9316, 9393, 9408, 9428, 9435, 9446, 9454, 9499, 9506, 9507, 9511,\n",
       "         9522, 9524, 9530, 9542, 9547, 9563, 9570, 9577, 9579, 9589, 9600,\n",
       "         9609, 9625, 9631, 9645, 9646, 9658, 9664, 9678, 9688, 9696, 9697,\n",
       "         9698, 9714, 9720, 9721, 9722, 9739, 9742, 9750, 9780, 9812, 9815,\n",
       "         9832, 9835, 9850, 9866, 9870, 9886, 9889, 9891, 9892, 9910, 9929,\n",
       "         9930, 9942, 9943, 9954, 9956, 9960, 9966, 9970])),\n",
       " (array([   0,    1,    2, ..., 9969, 9970, 9971]),\n",
       "  array([   3,    4,    7,   34,   47,   50,   79,   81,   83,   96,  109,\n",
       "          110,  120,  132,  134,  135,  141,  143,  149,  164,  170,  198,\n",
       "          214,  231,  232,  255,  257,  285,  288,  297,  314,  322,  327,\n",
       "          342,  351,  357,  369,  378,  381,  392,  427,  442,  443,  446,\n",
       "          455,  457,  463,  475,  487,  492,  494,  497,  502,  531,  533,\n",
       "          536,  540,  542,  544,  558,  587,  595,  600,  601,  605,  606,\n",
       "          607,  611,  614,  618,  624,  632,  640,  641,  646,  648,  655,\n",
       "          658,  662,  690,  702,  724,  726,  746,  749,  753,  761,  762,\n",
       "          765,  771,  776,  780,  786,  799,  815,  841,  850,  862,  872,\n",
       "          877,  902,  905,  910,  915,  932,  942,  962,  972,  978,  984,\n",
       "          989,  996, 1005, 1028, 1042, 1044, 1046, 1066, 1073, 1088, 1096,\n",
       "         1099, 1107, 1130, 1131, 1137, 1147, 1149, 1159, 1161, 1164, 1180,\n",
       "         1191, 1198, 1204, 1211, 1218, 1224, 1227, 1231, 1233, 1236, 1249,\n",
       "         1257, 1281, 1291, 1297, 1298, 1299, 1300, 1310, 1323, 1341, 1358,\n",
       "         1360, 1389, 1398, 1456, 1469, 1490, 1497, 1507, 1519, 1526, 1535,\n",
       "         1548, 1567, 1622, 1623, 1647, 1674, 1684, 1690, 1701, 1702, 1710,\n",
       "         1711, 1740, 1744, 1750, 1767, 1775, 1785, 1795, 1797, 1799, 1809,\n",
       "         1812, 1824, 1826, 1831, 1839, 1853, 1854, 1871, 1941, 1942, 1944,\n",
       "         1953, 1967, 1978, 1981, 1985, 2002, 2009, 2020, 2031, 2050, 2055,\n",
       "         2070, 2078, 2079, 2092, 2105, 2139, 2148, 2149, 2153, 2161, 2175,\n",
       "         2210, 2218, 2219, 2224, 2226, 2241, 2277, 2286, 2288, 2293, 2300,\n",
       "         2302, 2333, 2336, 2342, 2350, 2355, 2356, 2365, 2388, 2400, 2409,\n",
       "         2412, 2414, 2423, 2432, 2459, 2463, 2475, 2487, 2490, 2508, 2521,\n",
       "         2538, 2548, 2550, 2557, 2566, 2587, 2593, 2594, 2635, 2639, 2661,\n",
       "         2693, 2700, 2701, 2710, 2715, 2727, 2740, 2747, 2771, 2786, 2800,\n",
       "         2813, 2835, 2864, 2867, 2869, 2878, 2882, 2910, 2911, 2912, 2917,\n",
       "         2920, 2947, 2951, 2967, 2970, 2972, 2978, 2990, 2996, 3001, 3005,\n",
       "         3019, 3023, 3035, 3051, 3081, 3101, 3104, 3135, 3136, 3173, 3197,\n",
       "         3228, 3247, 3261, 3263, 3264, 3267, 3279, 3326, 3338, 3343, 3350,\n",
       "         3360, 3361, 3373, 3398, 3405, 3414, 3417, 3431, 3433, 3443, 3465,\n",
       "         3466, 3474, 3483, 3491, 3506, 3518, 3523, 3528, 3533, 3540, 3556,\n",
       "         3560, 3566, 3568, 3574, 3576, 3605, 3615, 3629, 3630, 3647, 3667,\n",
       "         3683, 3684, 3694, 3695, 3703, 3710, 3716, 3717, 3721, 3730, 3742,\n",
       "         3751, 3754, 3755, 3766, 3779, 3790, 3811, 3818, 3821, 3828, 3830,\n",
       "         3838, 3866, 3869, 3870, 3871, 3881, 3888, 3890, 3911, 3926, 3942,\n",
       "         3948, 3949, 3956, 3966, 3975, 3996, 4000, 4011, 4038, 4044, 4046,\n",
       "         4073, 4077, 4088, 4089, 4096, 4112, 4114, 4125, 4129, 4136, 4146,\n",
       "         4150, 4157, 4164, 4171, 4179, 4191, 4210, 4218, 4223, 4243, 4249,\n",
       "         4281, 4282, 4284, 4327, 4333, 4342, 4374, 4392, 4398, 4400, 4408,\n",
       "         4416, 4427, 4429, 4430, 4468, 4473, 4477, 4482, 4484, 4517, 4531,\n",
       "         4549, 4557, 4562, 4563, 4570, 4571, 4579, 4582, 4591, 4593, 4597,\n",
       "         4615, 4617, 4627, 4647, 4652, 4664, 4667, 4670, 4694, 4700, 4711,\n",
       "         4713, 4716, 4727, 4738, 4752, 4758, 4761, 4764, 4772, 4781, 4785,\n",
       "         4794, 4798, 4808, 4810, 4815, 4816, 4817, 4819, 4827, 4840, 4855,\n",
       "         4868, 4879, 4888, 4898, 4907, 4908, 4917, 4947, 4957, 4960, 4973,\n",
       "         4989, 5001, 5003, 5005, 5009, 5012, 5025, 5029, 5047, 5048, 5062,\n",
       "         5063, 5073, 5074, 5091, 5100, 5104, 5131, 5132, 5151, 5157, 5158,\n",
       "         5161, 5166, 5174, 5175, 5228, 5245, 5272, 5276, 5280, 5290, 5314,\n",
       "         5337, 5343, 5349, 5354, 5359, 5363, 5366, 5372, 5378, 5385, 5401,\n",
       "         5404, 5415, 5422, 5436, 5437, 5452, 5467, 5477, 5479, 5484, 5485,\n",
       "         5487, 5489, 5494, 5502, 5516, 5519, 5524, 5526, 5553, 5562, 5566,\n",
       "         5579, 5587, 5591, 5632, 5641, 5646, 5658, 5659, 5662, 5664, 5667,\n",
       "         5677, 5681, 5685, 5695, 5724, 5736, 5740, 5756, 5765, 5785, 5787,\n",
       "         5796, 5800, 5802, 5815, 5817, 5824, 5849, 5860, 5864, 5881, 5891,\n",
       "         5903, 5907, 5926, 5947, 5959, 5963, 5968, 5984, 5988, 5990, 6019,\n",
       "         6025, 6030, 6035, 6050, 6053, 6063, 6068, 6087, 6109, 6112, 6113,\n",
       "         6119, 6121, 6143, 6146, 6149, 6157, 6158, 6162, 6167, 6171, 6177,\n",
       "         6180, 6186, 6191, 6205, 6216, 6243, 6260, 6264, 6270, 6271, 6281,\n",
       "         6291, 6292, 6296, 6337, 6343, 6352, 6375, 6381, 6411, 6425, 6426,\n",
       "         6433, 6461, 6481, 6483, 6484, 6488, 6495, 6503, 6511, 6512, 6525,\n",
       "         6531, 6558, 6612, 6613, 6619, 6635, 6663, 6676, 6679, 6680, 6687,\n",
       "         6696, 6699, 6704, 6706, 6709, 6718, 6726, 6729, 6751, 6756, 6757,\n",
       "         6762, 6795, 6797, 6801, 6839, 6840, 6848, 6850, 6854, 6856, 6859,\n",
       "         6906, 6912, 6914, 6916, 6922, 6926, 6944, 6945, 6946, 6957, 6970,\n",
       "         6994, 6997, 7010, 7027, 7028, 7074, 7084, 7085, 7092, 7102, 7122,\n",
       "         7133, 7140, 7143, 7147, 7150, 7157, 7161, 7171, 7174, 7204, 7211,\n",
       "         7214, 7231, 7237, 7246, 7266, 7267, 7275, 7277, 7280, 7284, 7285,\n",
       "         7291, 7303, 7309, 7333, 7343, 7348, 7351, 7353, 7358, 7359, 7361,\n",
       "         7371, 7374, 7395, 7398, 7413, 7433, 7448, 7470, 7471, 7474, 7483,\n",
       "         7495, 7508, 7528, 7546, 7552, 7553, 7563, 7565, 7569, 7570, 7574,\n",
       "         7590, 7611, 7635, 7640, 7644, 7658, 7663, 7664, 7673, 7681, 7691,\n",
       "         7694, 7700, 7711, 7722, 7736, 7740, 7765, 7766, 7778, 7784, 7793,\n",
       "         7805, 7808, 7809, 7814, 7830, 7863, 7867, 7913, 7918, 7921, 7931,\n",
       "         7952, 7968, 7969, 7990, 7999, 8002, 8004, 8057, 8066, 8084, 8095,\n",
       "         8097, 8106, 8118, 8121, 8126, 8127, 8140, 8149, 8152, 8154, 8182,\n",
       "         8209, 8229, 8248, 8258, 8287, 8295, 8300, 8309, 8317, 8333, 8365,\n",
       "         8368, 8397, 8407, 8413, 8415, 8463, 8488, 8490, 8507, 8522, 8544,\n",
       "         8560, 8563, 8575, 8577, 8590, 8599, 8601, 8611, 8630, 8637, 8657,\n",
       "         8681, 8683, 8697, 8720, 8721, 8727, 8733, 8751, 8758, 8762, 8771,\n",
       "         8789, 8791, 8811, 8819, 8830, 8843, 8845, 8862, 8880, 8893, 8896,\n",
       "         8900, 8909, 8920, 8928, 8930, 8936, 8938, 8946, 8951, 8957, 8961,\n",
       "         8990, 8992, 9006, 9024, 9049, 9053, 9056, 9064, 9069, 9077, 9083,\n",
       "         9092, 9104, 9106, 9109, 9112, 9124, 9130, 9165, 9178, 9180, 9185,\n",
       "         9189, 9204, 9217, 9228, 9238, 9241, 9262, 9265, 9271, 9280, 9283,\n",
       "         9290, 9292, 9294, 9308, 9336, 9337, 9343, 9344, 9357, 9358, 9362,\n",
       "         9381, 9391, 9420, 9425, 9429, 9434, 9436, 9438, 9439, 9451, 9462,\n",
       "         9465, 9466, 9469, 9470, 9473, 9475, 9494, 9529, 9533, 9539, 9543,\n",
       "         9548, 9569, 9588, 9592, 9596, 9598, 9604, 9616, 9618, 9629, 9632,\n",
       "         9655, 9668, 9669, 9677, 9704, 9717, 9731, 9734, 9738, 9753, 9763,\n",
       "         9771, 9782, 9795, 9801, 9810, 9847, 9854, 9883, 9884, 9885, 9895,\n",
       "         9901, 9903, 9913, 9915, 9918, 9934, 9935])),\n",
       " (array([   0,    1,    2, ..., 9969, 9970, 9971]),\n",
       "  array([  10,   41,   60,   70,   76,   85,   90,   99,  106,  108,  125,\n",
       "          147,  151,  182,  197,  206,  211,  230,  240,  264,  280,  283,\n",
       "          287,  301,  349,  363,  365,  367,  380,  397,  400,  401,  406,\n",
       "          407,  417,  423,  430,  447,  474,  482,  499,  505,  522,  530,\n",
       "          537,  545,  559,  566,  568,  569,  570,  580,  581,  590,  593,\n",
       "          596,  599,  603,  609,  619,  629,  631,  636,  643,  644,  674,\n",
       "          675,  680,  687,  695,  730,  735,  741,  748,  766,  767,  773,\n",
       "          794,  795,  807,  811,  825,  834,  837,  876,  882,  885,  886,\n",
       "          916,  926,  935,  936,  943,  956,  971,  977,  985,  986, 1007,\n",
       "         1033, 1035, 1039, 1045, 1062, 1074, 1077, 1084, 1092, 1100, 1125,\n",
       "         1134, 1142, 1153, 1156, 1167, 1174, 1187, 1202, 1209, 1250, 1253,\n",
       "         1254, 1256, 1261, 1268, 1275, 1279, 1280, 1315, 1321, 1326, 1330,\n",
       "         1348, 1369, 1376, 1382, 1390, 1392, 1395, 1396, 1407, 1409, 1419,\n",
       "         1425, 1426, 1436, 1451, 1460, 1462, 1468, 1485, 1511, 1512, 1539,\n",
       "         1549, 1561, 1562, 1563, 1564, 1566, 1596, 1611, 1616, 1626, 1629,\n",
       "         1664, 1667, 1670, 1696, 1703, 1707, 1708, 1709, 1718, 1730, 1734,\n",
       "         1737, 1748, 1752, 1758, 1768, 1769, 1790, 1814, 1820, 1825, 1827,\n",
       "         1840, 1841, 1849, 1856, 1857, 1861, 1868, 1905, 1918, 1922, 1923,\n",
       "         1955, 1963, 1990, 2003, 2010, 2065, 2072, 2091, 2113, 2155, 2159,\n",
       "         2162, 2164, 2165, 2167, 2174, 2184, 2186, 2189, 2194, 2195, 2217,\n",
       "         2228, 2243, 2248, 2250, 2269, 2275, 2285, 2296, 2313, 2326, 2341,\n",
       "         2349, 2358, 2364, 2368, 2372, 2381, 2385, 2393, 2403, 2411, 2415,\n",
       "         2419, 2424, 2429, 2434, 2439, 2445, 2447, 2464, 2468, 2486, 2489,\n",
       "         2505, 2511, 2518, 2532, 2539, 2561, 2572, 2599, 2610, 2623, 2637,\n",
       "         2653, 2671, 2685, 2705, 2706, 2717, 2721, 2731, 2745, 2762, 2764,\n",
       "         2768, 2772, 2790, 2815, 2836, 2875, 2880, 2881, 2924, 2928, 2946,\n",
       "         2949, 2954, 2962, 2971, 2985, 2987, 3003, 3007, 3008, 3058, 3063,\n",
       "         3066, 3085, 3091, 3092, 3095, 3113, 3124, 3132, 3141, 3152, 3160,\n",
       "         3168, 3171, 3178, 3188, 3198, 3202, 3210, 3232, 3239, 3240, 3253,\n",
       "         3274, 3294, 3301, 3316, 3317, 3324, 3336, 3337, 3339, 3349, 3356,\n",
       "         3364, 3366, 3372, 3387, 3395, 3399, 3403, 3409, 3413, 3426, 3429,\n",
       "         3430, 3444, 3448, 3459, 3462, 3469, 3471, 3477, 3490, 3500, 3503,\n",
       "         3514, 3519, 3530, 3543, 3545, 3550, 3551, 3555, 3558, 3561, 3563,\n",
       "         3567, 3581, 3599, 3602, 3649, 3651, 3662, 3669, 3676, 3682, 3691,\n",
       "         3715, 3739, 3743, 3753, 3764, 3785, 3787, 3788, 3793, 3795, 3801,\n",
       "         3815, 3858, 3875, 3879, 3893, 3896, 3900, 3906, 3921, 3941, 3947,\n",
       "         3952, 3961, 3962, 3993, 3999, 4002, 4006, 4031, 4032, 4047, 4059,\n",
       "         4069, 4107, 4131, 4144, 4162, 4183, 4187, 4203, 4217, 4229, 4237,\n",
       "         4238, 4253, 4258, 4265, 4275, 4276, 4283, 4288, 4293, 4300, 4302,\n",
       "         4303, 4307, 4314, 4320, 4330, 4352, 4362, 4364, 4367, 4383, 4389,\n",
       "         4391, 4410, 4420, 4440, 4444, 4466, 4470, 4481, 4488, 4497, 4514,\n",
       "         4519, 4527, 4539, 4545, 4546, 4547, 4553, 4574, 4578, 4587, 4588,\n",
       "         4602, 4605, 4633, 4635, 4640, 4653, 4668, 4675, 4679, 4693, 4697,\n",
       "         4705, 4723, 4732, 4766, 4769, 4773, 4795, 4800, 4801, 4804, 4826,\n",
       "         4833, 4841, 4842, 4844, 4849, 4864, 4866, 4870, 4874, 4878, 4882,\n",
       "         4886, 4890, 4902, 4919, 4920, 4921, 4926, 4933, 4984, 5004, 5008,\n",
       "         5041, 5049, 5051, 5066, 5079, 5082, 5089, 5099, 5102, 5112, 5141,\n",
       "         5142, 5154, 5182, 5191, 5220, 5221, 5270, 5278, 5279, 5281, 5285,\n",
       "         5287, 5288, 5294, 5301, 5302, 5309, 5329, 5339, 5340, 5357, 5370,\n",
       "         5379, 5396, 5418, 5420, 5431, 5460, 5466, 5476, 5492, 5503, 5508,\n",
       "         5517, 5529, 5533, 5546, 5547, 5555, 5558, 5560, 5580, 5585, 5597,\n",
       "         5603, 5613, 5618, 5640, 5653, 5654, 5672, 5675, 5683, 5691, 5697,\n",
       "         5703, 5707, 5710, 5715, 5729, 5747, 5751, 5755, 5763, 5767, 5769,\n",
       "         5775, 5776, 5789, 5792, 5799, 5808, 5822, 5826, 5829, 5837, 5846,\n",
       "         5866, 5883, 5887, 5890, 5895, 5913, 5920, 5931, 5940, 5946, 5949,\n",
       "         5952, 5960, 5993, 5995, 6014, 6018, 6022, 6039, 6054, 6097, 6127,\n",
       "         6132, 6138, 6151, 6170, 6196, 6202, 6207, 6244, 6247, 6254, 6293,\n",
       "         6295, 6300, 6306, 6311, 6317, 6351, 6360, 6365, 6367, 6368, 6384,\n",
       "         6400, 6412, 6431, 6443, 6455, 6463, 6468, 6473, 6485, 6489, 6493,\n",
       "         6508, 6521, 6528, 6532, 6557, 6570, 6571, 6577, 6591, 6605, 6611,\n",
       "         6642, 6645, 6647, 6648, 6658, 6661, 6662, 6672, 6677, 6684, 6685,\n",
       "         6693, 6705, 6717, 6730, 6738, 6742, 6747, 6777, 6785, 6802, 6807,\n",
       "         6810, 6814, 6830, 6832, 6838, 6846, 6853, 6855, 6892, 6933, 7003,\n",
       "         7004, 7008, 7014, 7016, 7029, 7034, 7036, 7071, 7077, 7083, 7095,\n",
       "         7107, 7126, 7136, 7138, 7139, 7146, 7149, 7167, 7193, 7197, 7210,\n",
       "         7215, 7221, 7242, 7244, 7253, 7279, 7289, 7293, 7294, 7310, 7336,\n",
       "         7337, 7366, 7370, 7376, 7387, 7392, 7403, 7406, 7407, 7421, 7434,\n",
       "         7439, 7443, 7444, 7455, 7457, 7479, 7505, 7510, 7531, 7539, 7541,\n",
       "         7572, 7573, 7585, 7586, 7616, 7636, 7648, 7652, 7657, 7675, 7682,\n",
       "         7709, 7717, 7743, 7755, 7757, 7788, 7810, 7822, 7824, 7832, 7847,\n",
       "         7874, 7887, 7914, 7917, 7932, 7937, 7961, 7966, 7967, 7976, 7983,\n",
       "         7989, 8001, 8006, 8039, 8047, 8052, 8060, 8061, 8068, 8070, 8075,\n",
       "         8092, 8134, 8144, 8171, 8176, 8177, 8184, 8201, 8206, 8212, 8222,\n",
       "         8230, 8241, 8244, 8253, 8256, 8264, 8278, 8285, 8289, 8294, 8306,\n",
       "         8308, 8311, 8315, 8319, 8321, 8324, 8328, 8334, 8339, 8345, 8348,\n",
       "         8352, 8362, 8404, 8411, 8421, 8432, 8444, 8445, 8446, 8476, 8481,\n",
       "         8491, 8495, 8523, 8561, 8602, 8607, 8608, 8635, 8636, 8638, 8641,\n",
       "         8648, 8661, 8668, 8674, 8698, 8700, 8707, 8715, 8716, 8719, 8734,\n",
       "         8741, 8744, 8759, 8761, 8765, 8788, 8796, 8820, 8856, 8866, 8878,\n",
       "         8890, 8901, 8908, 8914, 8921, 8927, 8947, 8949, 8954, 8959, 8964,\n",
       "         8983, 8998, 9002, 9010, 9011, 9017, 9028, 9045, 9046, 9054, 9065,\n",
       "         9093, 9102, 9111, 9138, 9143, 9154, 9155, 9156, 9159, 9162, 9174,\n",
       "         9201, 9206, 9219, 9223, 9231, 9237, 9242, 9248, 9257, 9268, 9281,\n",
       "         9286, 9302, 9320, 9327, 9346, 9353, 9364, 9375, 9384, 9400, 9401,\n",
       "         9409, 9412, 9426, 9440, 9456, 9459, 9464, 9472, 9474, 9493, 9500,\n",
       "         9514, 9516, 9518, 9523, 9531, 9546, 9552, 9553, 9554, 9564, 9572,\n",
       "         9573, 9587, 9608, 9611, 9617, 9636, 9643, 9644, 9654, 9657, 9665,\n",
       "         9680, 9691, 9707, 9709, 9729, 9735, 9744, 9756, 9757, 9765, 9773,\n",
       "         9779, 9791, 9792, 9804, 9823, 9830, 9831, 9839, 9845, 9849, 9852,\n",
       "         9855, 9857, 9860, 9862, 9871, 9879, 9881, 9882, 9899, 9900, 9907,\n",
       "         9925, 9928, 9937, 9944, 9945, 9959, 9961])),\n",
       " (array([   0,    1,    2, ..., 9969, 9970, 9971]),\n",
       "  array([  23,   30,   35,   39,   48,   56,   64,   77,   82,  114,  119,\n",
       "          124,  126,  128,  163,  166,  173,  184,  189,  192,  208,  226,\n",
       "          233,  242,  253,  259,  262,  268,  277,  286,  298,  302,  317,\n",
       "          318,  330,  332,  343,  347,  348,  396,  410,  421,  425,  434,\n",
       "          450,  461,  466,  485,  489,  501,  504,  506,  512,  519,  523,\n",
       "          525,  528,  529,  538,  552,  553,  574,  586,  597,  638,  650,\n",
       "          672,  681,  698,  713,  714,  715,  720,  734,  739,  740,  743,\n",
       "          751,  752,  757,  768,  772,  793,  796,  797,  802,  803,  820,\n",
       "          824,  827,  828,  853,  860,  867,  879,  888,  909,  919,  937,\n",
       "          960,  976,  979,  994,  995,  997, 1014, 1021, 1030, 1038, 1041,\n",
       "         1052, 1060, 1061, 1067, 1069, 1075, 1113, 1128, 1143, 1151, 1157,\n",
       "         1168, 1189, 1212, 1213, 1238, 1248, 1259, 1270, 1278, 1283, 1284,\n",
       "         1286, 1288, 1294, 1301, 1320, 1325, 1338, 1342, 1345, 1353, 1354,\n",
       "         1365, 1367, 1399, 1413, 1417, 1423, 1454, 1475, 1481, 1496, 1500,\n",
       "         1503, 1505, 1506, 1510, 1514, 1527, 1528, 1529, 1541, 1544, 1550,\n",
       "         1556, 1569, 1592, 1604, 1615, 1635, 1641, 1648, 1652, 1653, 1655,\n",
       "         1657, 1658, 1662, 1665, 1671, 1683, 1691, 1704, 1719, 1721, 1728,\n",
       "         1756, 1779, 1788, 1792, 1804, 1807, 1817, 1836, 1838, 1842, 1864,\n",
       "         1865, 1873, 1879, 1886, 1889, 1906, 1920, 1930, 1931, 1933, 1971,\n",
       "         1977, 1991, 2004, 2005, 2011, 2014, 2017, 2024, 2025, 2032, 2066,\n",
       "         2069, 2082, 2086, 2088, 2089, 2116, 2122, 2126, 2131, 2140, 2171,\n",
       "         2181, 2185, 2190, 2227, 2235, 2242, 2276, 2314, 2315, 2319, 2320,\n",
       "         2324, 2344, 2359, 2361, 2366, 2371, 2377, 2383, 2389, 2401, 2410,\n",
       "         2456, 2461, 2470, 2485, 2522, 2526, 2551, 2554, 2560, 2571, 2575,\n",
       "         2585, 2588, 2604, 2609, 2616, 2618, 2632, 2651, 2667, 2673, 2680,\n",
       "         2690, 2694, 2711, 2716, 2720, 2723, 2736, 2757, 2760, 2766, 2776,\n",
       "         2778, 2779, 2785, 2794, 2796, 2811, 2831, 2838, 2856, 2861, 2876,\n",
       "         2883, 2902, 2909, 2934, 2948, 2950, 2960, 2977, 2992, 2993, 2998,\n",
       "         3006, 3014, 3021, 3032, 3045, 3057, 3065, 3067, 3077, 3079, 3099,\n",
       "         3111, 3145, 3147, 3153, 3172, 3181, 3194, 3195, 3203, 3207, 3212,\n",
       "         3217, 3237, 3238, 3265, 3271, 3273, 3278, 3291, 3297, 3307, 3311,\n",
       "         3315, 3318, 3325, 3331, 3333, 3334, 3335, 3368, 3369, 3375, 3404,\n",
       "         3410, 3421, 3438, 3456, 3460, 3461, 3468, 3479, 3489, 3495, 3498,\n",
       "         3521, 3527, 3539, 3544, 3548, 3553, 3569, 3595, 3607, 3609, 3637,\n",
       "         3657, 3660, 3663, 3671, 3685, 3719, 3725, 3752, 3757, 3773, 3784,\n",
       "         3797, 3800, 3817, 3823, 3826, 3827, 3839, 3846, 3851, 3862, 3882,\n",
       "         3899, 3902, 3904, 3909, 3932, 3935, 3936, 3939, 3946, 3957, 3960,\n",
       "         3963, 3970, 3977, 3984, 3989, 4008, 4010, 4018, 4029, 4037, 4051,\n",
       "         4064, 4066, 4072, 4097, 4108, 4122, 4135, 4151, 4160, 4165, 4166,\n",
       "         4180, 4188, 4189, 4196, 4201, 4206, 4208, 4209, 4214, 4216, 4247,\n",
       "         4261, 4262, 4264, 4279, 4286, 4287, 4295, 4306, 4326, 4385, 4395,\n",
       "         4399, 4401, 4431, 4457, 4460, 4486, 4494, 4501, 4503, 4504, 4521,\n",
       "         4526, 4554, 4561, 4566, 4586, 4599, 4611, 4637, 4651, 4665, 4677,\n",
       "         4687, 4698, 4699, 4702, 4709, 4737, 4746, 4767, 4768, 4776, 4793,\n",
       "         4820, 4838, 4869, 4881, 4884, 4923, 4927, 4928, 4930, 4934, 4938,\n",
       "         4940, 4941, 4942, 4970, 4972, 4976, 4985, 4992, 5022, 5094, 5108,\n",
       "         5109, 5111, 5116, 5120, 5147, 5153, 5159, 5164, 5167, 5171, 5189,\n",
       "         5193, 5194, 5206, 5208, 5216, 5237, 5238, 5252, 5253, 5264, 5271,\n",
       "         5284, 5369, 5373, 5377, 5382, 5390, 5398, 5429, 5443, 5451, 5456,\n",
       "         5474, 5478, 5480, 5493, 5501, 5509, 5512, 5520, 5522, 5542, 5544,\n",
       "         5550, 5568, 5574, 5575, 5577, 5581, 5582, 5593, 5595, 5598, 5599,\n",
       "         5607, 5623, 5627, 5644, 5649, 5694, 5699, 5718, 5721, 5743, 5750,\n",
       "         5757, 5771, 5773, 5781, 5791, 5798, 5807, 5842, 5882, 5889, 5892,\n",
       "         5897, 5914, 5929, 5932, 5944, 5955, 5971, 5973, 5997, 6017, 6026,\n",
       "         6040, 6060, 6075, 6079, 6107, 6108, 6110, 6120, 6136, 6160, 6178,\n",
       "         6192, 6194, 6197, 6210, 6213, 6226, 6238, 6239, 6261, 6267, 6276,\n",
       "         6285, 6298, 6307, 6308, 6312, 6318, 6330, 6340, 6342, 6362, 6372,\n",
       "         6378, 6379, 6380, 6394, 6414, 6416, 6420, 6427, 6434, 6436, 6439,\n",
       "         6444, 6450, 6456, 6472, 6501, 6513, 6515, 6518, 6523, 6545, 6553,\n",
       "         6562, 6576, 6579, 6586, 6587, 6588, 6595, 6608, 6616, 6620, 6626,\n",
       "         6629, 6637, 6638, 6644, 6653, 6665, 6675, 6678, 6701, 6703, 6722,\n",
       "         6749, 6763, 6764, 6765, 6800, 6820, 6826, 6836, 6842, 6843, 6844,\n",
       "         6845, 6861, 6863, 6871, 6876, 6877, 6883, 6884, 6897, 6903, 6905,\n",
       "         6918, 6920, 6939, 6940, 6941, 6952, 6964, 6966, 6971, 6972, 6982,\n",
       "         6985, 6991, 6998, 7020, 7025, 7031, 7038, 7059, 7093, 7108, 7131,\n",
       "         7132, 7134, 7142, 7162, 7170, 7172, 7177, 7186, 7196, 7200, 7207,\n",
       "         7208, 7217, 7219, 7228, 7240, 7268, 7273, 7302, 7315, 7331, 7334,\n",
       "         7340, 7342, 7345, 7347, 7384, 7390, 7391, 7400, 7424, 7431, 7438,\n",
       "         7446, 7450, 7458, 7461, 7463, 7484, 7490, 7491, 7499, 7500, 7502,\n",
       "         7540, 7558, 7559, 7564, 7566, 7591, 7592, 7594, 7622, 7627, 7629,\n",
       "         7637, 7647, 7676, 7688, 7695, 7696, 7701, 7707, 7732, 7737, 7751,\n",
       "         7752, 7759, 7774, 7783, 7791, 7812, 7821, 7839, 7843, 7846, 7884,\n",
       "         7892, 7893, 7897, 7900, 7928, 7934, 7939, 7940, 7941, 7972, 7981,\n",
       "         8000, 8009, 8010, 8019, 8020, 8031, 8036, 8038, 8044, 8045, 8056,\n",
       "         8058, 8071, 8086, 8087, 8096, 8098, 8100, 8103, 8112, 8117, 8125,\n",
       "         8132, 8133, 8138, 8147, 8153, 8157, 8159, 8167, 8194, 8197, 8200,\n",
       "         8205, 8215, 8221, 8238, 8247, 8254, 8261, 8270, 8275, 8277, 8279,\n",
       "         8301, 8310, 8314, 8323, 8349, 8359, 8363, 8378, 8385, 8399, 8423,\n",
       "         8439, 8458, 8460, 8482, 8483, 8499, 8521, 8554, 8557, 8568, 8604,\n",
       "         8606, 8628, 8642, 8644, 8647, 8649, 8655, 8666, 8691, 8693, 8695,\n",
       "         8696, 8699, 8703, 8708, 8763, 8770, 8775, 8785, 8804, 8817, 8827,\n",
       "         8837, 8840, 8847, 8851, 8859, 8867, 8868, 8872, 8884, 8886, 8897,\n",
       "         8907, 8922, 8923, 8948, 8960, 8970, 8971, 8975, 8978, 8985, 8987,\n",
       "         9007, 9018, 9020, 9021, 9031, 9060, 9068, 9078, 9080, 9082, 9119,\n",
       "         9131, 9133, 9148, 9152, 9163, 9171, 9179, 9186, 9199, 9200, 9203,\n",
       "         9209, 9214, 9225, 9235, 9239, 9272, 9276, 9279, 9288, 9300, 9301,\n",
       "         9309, 9339, 9340, 9348, 9366, 9371, 9373, 9389, 9392, 9395, 9407,\n",
       "         9411, 9445, 9476, 9512, 9519, 9534, 9540, 9555, 9568, 9591, 9601,\n",
       "         9610, 9630, 9634, 9639, 9650, 9660, 9679, 9687, 9692, 9695, 9712,\n",
       "         9716, 9737, 9741, 9749, 9767, 9769, 9781, 9784, 9811, 9820, 9841,\n",
       "         9853, 9867, 9875, 9877, 9878, 9922, 9950])),\n",
       " (array([   0,    1,    2, ..., 9969, 9970, 9971]),\n",
       "  array([   8,   16,   24,   36,   38,   51,   53,   59,   61,   87,   89,\n",
       "          127,  136,  152,  172,  199,  200,  209,  218,  220,  227,  228,\n",
       "          244,  249,  258,  267,  269,  272,  289,  300,  319,  321,  324,\n",
       "          337,  341,  353,  354,  413,  416,  429,  435,  460,  473,  573,\n",
       "          575,  583,  589,  635,  645,  656,  679,  688,  691,  694,  699,\n",
       "          700,  723,  727,  729,  732,  736,  758,  810,  869,  880,  881,\n",
       "          911,  912,  913,  924,  947,  954,  964,  980, 1008, 1032, 1037,\n",
       "         1043, 1055, 1057, 1059, 1079, 1087, 1097, 1118, 1126, 1129, 1132,\n",
       "         1144, 1169, 1172, 1175, 1178, 1196, 1206, 1217, 1221, 1228, 1232,\n",
       "         1243, 1246, 1252, 1264, 1265, 1272, 1304, 1305, 1327, 1347, 1349,\n",
       "         1352, 1368, 1383, 1385, 1387, 1414, 1422, 1429, 1442, 1443, 1444,\n",
       "         1448, 1457, 1467, 1471, 1491, 1494, 1502, 1530, 1531, 1542, 1543,\n",
       "         1547, 1553, 1560, 1574, 1575, 1577, 1599, 1649, 1659, 1661, 1666,\n",
       "         1669, 1677, 1688, 1689, 1705, 1706, 1714, 1723, 1724, 1741, 1751,\n",
       "         1755, 1761, 1773, 1776, 1786, 1821, 1850, 1876, 1877, 1878, 1899,\n",
       "         1932, 1962, 1965, 1968, 1980, 1994, 1996, 2026, 2027, 2029, 2054,\n",
       "         2061, 2095, 2109, 2119, 2134, 2145, 2156, 2168, 2176, 2188, 2209,\n",
       "         2212, 2222, 2223, 2229, 2230, 2233, 2237, 2254, 2266, 2291, 2292,\n",
       "         2298, 2308, 2310, 2318, 2331, 2352, 2357, 2367, 2374, 2375, 2382,\n",
       "         2386, 2408, 2413, 2417, 2427, 2428, 2433, 2435, 2436, 2452, 2455,\n",
       "         2471, 2480, 2482, 2493, 2494, 2495, 2497, 2499, 2500, 2506, 2514,\n",
       "         2529, 2543, 2549, 2555, 2564, 2569, 2590, 2592, 2613, 2617, 2628,\n",
       "         2630, 2640, 2645, 2656, 2662, 2674, 2683, 2691, 2699, 2702, 2719,\n",
       "         2722, 2726, 2756, 2765, 2769, 2770, 2773, 2775, 2784, 2805, 2809,\n",
       "         2825, 2834, 2846, 2853, 2871, 2873, 2874, 2886, 2887, 2888, 2890,\n",
       "         2901, 2926, 2927, 2935, 2936, 2942, 2945, 2953, 2959, 2979, 2986,\n",
       "         2989, 2999, 3020, 3025, 3038, 3050, 3060, 3064, 3070, 3078, 3088,\n",
       "         3089, 3090, 3100, 3115, 3121, 3130, 3140, 3150, 3161, 3167, 3184,\n",
       "         3201, 3214, 3218, 3230, 3231, 3250, 3257, 3260, 3276, 3287, 3309,\n",
       "         3314, 3328, 3354, 3359, 3380, 3382, 3386, 3392, 3401, 3408, 3419,\n",
       "         3436, 3442, 3451, 3467, 3494, 3529, 3546, 3549, 3562, 3564, 3583,\n",
       "         3588, 3590, 3594, 3601, 3618, 3636, 3638, 3646, 3648, 3655, 3659,\n",
       "         3678, 3686, 3705, 3711, 3724, 3736, 3762, 3763, 3789, 3799, 3805,\n",
       "         3808, 3852, 3853, 3868, 3874, 3891, 3892, 3897, 3901, 3912, 3913,\n",
       "         3916, 3917, 3928, 3943, 3965, 3985, 3988, 3990, 4003, 4005, 4013,\n",
       "         4014, 4020, 4034, 4058, 4065, 4075, 4080, 4084, 4094, 4104, 4123,\n",
       "         4126, 4130, 4137, 4145, 4156, 4194, 4202, 4205, 4212, 4221, 4224,\n",
       "         4228, 4230, 4231, 4241, 4244, 4250, 4252, 4255, 4270, 4271, 4290,\n",
       "         4291, 4299, 4308, 4309, 4315, 4323, 4337, 4346, 4355, 4356, 4358,\n",
       "         4368, 4387, 4404, 4411, 4417, 4451, 4464, 4471, 4478, 4489, 4492,\n",
       "         4524, 4542, 4550, 4552, 4555, 4560, 4572, 4575, 4589, 4598, 4600,\n",
       "         4608, 4609, 4614, 4618, 4622, 4630, 4636, 4646, 4648, 4656, 4663,\n",
       "         4672, 4674, 4685, 4691, 4703, 4710, 4725, 4726, 4730, 4733, 4734,\n",
       "         4748, 4753, 4755, 4760, 4770, 4792, 4807, 4837, 4853, 4856, 4859,\n",
       "         4863, 4889, 4897, 4910, 4924, 4937, 4943, 4949, 4954, 4966, 4980,\n",
       "         5002, 5006, 5014, 5018, 5023, 5035, 5038, 5061, 5065, 5090, 5093,\n",
       "         5098, 5105, 5123, 5128, 5133, 5134, 5137, 5146, 5152, 5173, 5184,\n",
       "         5192, 5202, 5215, 5217, 5239, 5241, 5262, 5291, 5306, 5310, 5315,\n",
       "         5317, 5321, 5334, 5335, 5347, 5350, 5355, 5360, 5362, 5365, 5376,\n",
       "         5389, 5400, 5402, 5427, 5445, 5459, 5461, 5506, 5530, 5532, 5535,\n",
       "         5545, 5548, 5559, 5561, 5588, 5626, 5636, 5643, 5679, 5700, 5704,\n",
       "         5711, 5713, 5716, 5722, 5744, 5780, 5812, 5821, 5831, 5836, 5839,\n",
       "         5843, 5855, 5863, 5875, 5879, 5885, 5894, 5904, 5916, 5941, 5954,\n",
       "         5958, 5969, 5992, 6029, 6033, 6036, 6046, 6055, 6057, 6064, 6066,\n",
       "         6085, 6091, 6100, 6142, 6150, 6155, 6163, 6165, 6176, 6179, 6181,\n",
       "         6188, 6190, 6208, 6211, 6229, 6237, 6242, 6250, 6259, 6265, 6299,\n",
       "         6305, 6322, 6325, 6349, 6370, 6371, 6383, 6391, 6398, 6402, 6403,\n",
       "         6404, 6406, 6409, 6410, 6440, 6466, 6500, 6505, 6539, 6552, 6566,\n",
       "         6567, 6575, 6606, 6624, 6627, 6639, 6643, 6646, 6666, 6668, 6673,\n",
       "         6681, 6690, 6691, 6694, 6725, 6728, 6736, 6737, 6740, 6750, 6778,\n",
       "         6782, 6790, 6798, 6806, 6812, 6816, 6818, 6819, 6821, 6828, 6841,\n",
       "         6858, 6874, 6885, 6895, 6908, 6910, 6915, 6921, 6930, 6937, 6943,\n",
       "         6961, 6995, 6999, 7039, 7045, 7047, 7061, 7062, 7068, 7072, 7078,\n",
       "         7080, 7097, 7109, 7120, 7125, 7137, 7153, 7163, 7168, 7184, 7187,\n",
       "         7212, 7222, 7229, 7238, 7239, 7241, 7249, 7254, 7260, 7305, 7307,\n",
       "         7324, 7330, 7346, 7350, 7368, 7375, 7397, 7399, 7408, 7409, 7422,\n",
       "         7423, 7436, 7442, 7454, 7464, 7482, 7487, 7492, 7494, 7509, 7514,\n",
       "         7518, 7535, 7578, 7589, 7597, 7617, 7634, 7653, 7674, 7684, 7687,\n",
       "         7692, 7693, 7705, 7715, 7716, 7718, 7723, 7760, 7770, 7794, 7795,\n",
       "         7804, 7816, 7819, 7823, 7825, 7838, 7860, 7878, 7894, 7898, 7907,\n",
       "         7908, 7909, 7911, 7915, 7927, 7938, 7946, 7950, 7957, 7963, 7985,\n",
       "         7991, 7992, 8012, 8016, 8021, 8023, 8025, 8040, 8059, 8076, 8078,\n",
       "         8089, 8091, 8108, 8128, 8146, 8148, 8151, 8156, 8169, 8170, 8187,\n",
       "         8189, 8192, 8195, 8208, 8210, 8213, 8239, 8240, 8245, 8260, 8272,\n",
       "         8280, 8283, 8298, 8331, 8336, 8347, 8355, 8356, 8358, 8364, 8377,\n",
       "         8382, 8396, 8403, 8406, 8429, 8438, 8452, 8467, 8480, 8487, 8494,\n",
       "         8502, 8510, 8515, 8517, 8525, 8552, 8558, 8587, 8598, 8600, 8621,\n",
       "         8660, 8662, 8673, 8680, 8687, 8713, 8717, 8722, 8724, 8760, 8764,\n",
       "         8780, 8782, 8787, 8790, 8806, 8815, 8816, 8818, 8823, 8836, 8848,\n",
       "         8853, 8863, 8870, 8881, 8892, 8898, 8911, 8912, 8915, 8918, 8939,\n",
       "         8940, 8952, 8953, 8966, 8967, 8968, 8974, 8995, 9022, 9027, 9030,\n",
       "         9036, 9038, 9042, 9051, 9055, 9061, 9076, 9081, 9084, 9094, 9100,\n",
       "         9105, 9108, 9120, 9121, 9125, 9139, 9140, 9146, 9153, 9166, 9175,\n",
       "         9176, 9190, 9192, 9196, 9197, 9221, 9222, 9224, 9251, 9252, 9287,\n",
       "         9291, 9299, 9304, 9307, 9313, 9319, 9334, 9335, 9351, 9377, 9387,\n",
       "         9394, 9397, 9399, 9403, 9410, 9418, 9423, 9443, 9448, 9449, 9479,\n",
       "         9483, 9489, 9491, 9492, 9515, 9528, 9561, 9565, 9571, 9590, 9593,\n",
       "         9613, 9621, 9635, 9640, 9648, 9653, 9656, 9662, 9667, 9674, 9694,\n",
       "         9705, 9732, 9758, 9760, 9776, 9777, 9797, 9803, 9816, 9836, 9838,\n",
       "         9844, 9846, 9848, 9861, 9865, 9872, 9874, 9880, 9896, 9897, 9902,\n",
       "         9908, 9912, 9927, 9940, 9955, 9965, 9967])),\n",
       " (array([   1,    2,    3, ..., 9969, 9970, 9971]),\n",
       "  array([   0,   22,   25,   27,   49,   62,   93,   94,  113,  118,  133,\n",
       "          140,  148,  176,  195,  202,  219,  223,  235,  236,  239,  271,\n",
       "          274,  275,  299,  326,  334,  339,  346,  355,  358,  368,  375,\n",
       "          383,  412,  414,  420,  431,  440,  444,  448,  464,  468,  469,\n",
       "          479,  484,  486,  490,  493,  511,  515,  534,  548,  551,  560,\n",
       "          567,  572,  582,  588,  591,  592,  598,  627,  653,  664,  671,\n",
       "          678,  682,  685,  689,  701,  716,  782,  787,  814,  826,  842,\n",
       "          846,  861,  865,  875,  884,  893,  907,  921,  948,  952,  990,\n",
       "         1009, 1010, 1011, 1012, 1024, 1029, 1034, 1036, 1040, 1065, 1114,\n",
       "         1119, 1135, 1139, 1145, 1146, 1150, 1184, 1190, 1194, 1199, 1208,\n",
       "         1214, 1226, 1241, 1258, 1263, 1269, 1271, 1282, 1312, 1317, 1322,\n",
       "         1329, 1334, 1335, 1336, 1339, 1343, 1346, 1350, 1364, 1370, 1373,\n",
       "         1377, 1378, 1386, 1391, 1403, 1408, 1418, 1427, 1430, 1434, 1438,\n",
       "         1465, 1488, 1489, 1520, 1521, 1522, 1540, 1565, 1571, 1589, 1590,\n",
       "         1598, 1601, 1613, 1617, 1618, 1630, 1634, 1636, 1643, 1645, 1694,\n",
       "         1715, 1716, 1731, 1742, 1759, 1765, 1771, 1806, 1813, 1830, 1834,\n",
       "         1848, 1872, 1891, 1901, 1902, 1913, 1915, 1921, 1924, 1926, 1954,\n",
       "         1983, 1992, 1995, 2015, 2030, 2038, 2042, 2058, 2064, 2080, 2094,\n",
       "         2096, 2107, 2120, 2123, 2127, 2141, 2142, 2147, 2160, 2163, 2169,\n",
       "         2170, 2172, 2191, 2200, 2205, 2211, 2220, 2236, 2246, 2247, 2251,\n",
       "         2258, 2262, 2267, 2268, 2271, 2282, 2306, 2312, 2325, 2345, 2347,\n",
       "         2351, 2384, 2391, 2392, 2397, 2404, 2416, 2440, 2441, 2444, 2451,\n",
       "         2469, 2473, 2498, 2507, 2509, 2510, 2512, 2515, 2535, 2542, 2556,\n",
       "         2568, 2570, 2579, 2586, 2602, 2605, 2612, 2614, 2638, 2644, 2649,\n",
       "         2658, 2664, 2676, 2679, 2686, 2689, 2698, 2707, 2714, 2741, 2748,\n",
       "         2751, 2759, 2792, 2804, 2806, 2812, 2826, 2827, 2837, 2842, 2847,\n",
       "         2850, 2858, 2877, 2907, 2930, 2931, 2940, 2957, 2961, 2973, 2980,\n",
       "         2991, 2995, 3027, 3030, 3034, 3039, 3043, 3049, 3052, 3055, 3061,\n",
       "         3068, 3072, 3074, 3084, 3094, 3097, 3098, 3114, 3116, 3125, 3148,\n",
       "         3177, 3186, 3189, 3190, 3200, 3205, 3211, 3213, 3224, 3226, 3233,\n",
       "         3241, 3275, 3280, 3284, 3289, 3295, 3303, 3305, 3306, 3320, 3321,\n",
       "         3327, 3340, 3348, 3351, 3353, 3358, 3397, 3406, 3411, 3412, 3420,\n",
       "         3432, 3440, 3446, 3447, 3449, 3455, 3478, 3481, 3484, 3487, 3496,\n",
       "         3508, 3517, 3531, 3542, 3554, 3575, 3578, 3584, 3589, 3593, 3614,\n",
       "         3620, 3623, 3631, 3633, 3645, 3650, 3653, 3658, 3692, 3699, 3704,\n",
       "         3706, 3707, 3722, 3728, 3731, 3733, 3734, 3740, 3741, 3744, 3747,\n",
       "         3748, 3765, 3769, 3772, 3774, 3814, 3822, 3833, 3836, 3841, 3845,\n",
       "         3848, 3878, 3914, 3929, 3931, 3955, 3968, 3981, 3995, 4001, 4017,\n",
       "         4023, 4024, 4033, 4042, 4057, 4067, 4081, 4105, 4106, 4120, 4178,\n",
       "         4186, 4195, 4207, 4215, 4220, 4226, 4266, 4273, 4274, 4292, 4298,\n",
       "         4301, 4312, 4313, 4317, 4318, 4319, 4321, 4365, 4375, 4384, 4386,\n",
       "         4406, 4407, 4413, 4425, 4439, 4456, 4458, 4461, 4490, 4507, 4515,\n",
       "         4520, 4523, 4541, 4551, 4567, 4594, 4606, 4632, 4649, 4655, 4669,\n",
       "         4676, 4690, 4740, 4747, 4763, 4771, 4774, 4784, 4821, 4850, 4852,\n",
       "         4854, 4871, 4875, 4885, 4891, 4893, 4900, 4901, 4909, 4916, 4925,\n",
       "         4956, 4958, 4963, 4969, 4983, 4999, 5021, 5030, 5044, 5083, 5086,\n",
       "         5107, 5110, 5115, 5129, 5130, 5145, 5155, 5160, 5165, 5177, 5183,\n",
       "         5190, 5226, 5234, 5240, 5266, 5268, 5274, 5289, 5295, 5303, 5336,\n",
       "         5346, 5353, 5408, 5411, 5432, 5447, 5449, 5450, 5454, 5458, 5463,\n",
       "         5482, 5500, 5527, 5540, 5543, 5590, 5596, 5601, 5610, 5615, 5620,\n",
       "         5624, 5647, 5650, 5655, 5669, 5682, 5693, 5696, 5698, 5705, 5706,\n",
       "         5719, 5734, 5741, 5754, 5768, 5774, 5803, 5805, 5838, 5851, 5859,\n",
       "         5862, 5868, 5878, 5906, 5908, 5921, 5933, 5951, 5957, 5966, 5967,\n",
       "         5972, 5976, 5979, 5981, 5996, 5998, 6016, 6037, 6047, 6058, 6071,\n",
       "         6078, 6084, 6090, 6098, 6102, 6104, 6144, 6200, 6212, 6217, 6221,\n",
       "         6230, 6236, 6240, 6245, 6246, 6257, 6275, 6284, 6294, 6302, 6315,\n",
       "         6319, 6320, 6324, 6331, 6332, 6336, 6339, 6341, 6345, 6355, 6359,\n",
       "         6366, 6405, 6407, 6418, 6428, 6446, 6454, 6462, 6467, 6490, 6492,\n",
       "         6506, 6514, 6520, 6522, 6537, 6540, 6543, 6555, 6559, 6563, 6568,\n",
       "         6593, 6599, 6603, 6623, 6628, 6640, 6669, 6670, 6688, 6698, 6700,\n",
       "         6710, 6713, 6714, 6716, 6734, 6759, 6773, 6774, 6783, 6788, 6817,\n",
       "         6825, 6833, 6837, 6869, 6898, 6902, 6927, 6942, 6949, 6960, 6967,\n",
       "         6989, 6992, 6993, 7007, 7018, 7026, 7032, 7044, 7046, 7056, 7057,\n",
       "         7060, 7065, 7081, 7091, 7096, 7098, 7113, 7116, 7121, 7141, 7151,\n",
       "         7165, 7166, 7169, 7179, 7181, 7201, 7213, 7218, 7245, 7250, 7259,\n",
       "         7292, 7298, 7311, 7314, 7323, 7327, 7328, 7338, 7355, 7363, 7369,\n",
       "         7377, 7379, 7382, 7394, 7412, 7414, 7429, 7435, 7440, 7451, 7467,\n",
       "         7468, 7473, 7480, 7485, 7489, 7503, 7522, 7524, 7526, 7530, 7538,\n",
       "         7543, 7579, 7581, 7582, 7612, 7615, 7623, 7632, 7633, 7660, 7662,\n",
       "         7689, 7697, 7699, 7706, 7710, 7712, 7713, 7721, 7724, 7731, 7748,\n",
       "         7749, 7768, 7781, 7785, 7787, 7798, 7811, 7817, 7818, 7828, 7836,\n",
       "         7859, 7873, 7876, 7886, 7902, 7905, 7945, 7948, 7949, 7954, 7959,\n",
       "         7965, 7986, 7998, 8022, 8024, 8029, 8035, 8065, 8079, 8088, 8104,\n",
       "         8123, 8137, 8145, 8150, 8160, 8173, 8185, 8190, 8204, 8214, 8220,\n",
       "         8226, 8234, 8236, 8252, 8266, 8271, 8274, 8276, 8284, 8286, 8290,\n",
       "         8299, 8316, 8337, 8343, 8344, 8367, 8375, 8388, 8410, 8418, 8419,\n",
       "         8424, 8425, 8430, 8447, 8457, 8470, 8479, 8513, 8516, 8518, 8524,\n",
       "         8528, 8531, 8534, 8542, 8547, 8567, 8572, 8585, 8593, 8597, 8612,\n",
       "         8613, 8615, 8623, 8624, 8639, 8640, 8658, 8677, 8686, 8694, 8711,\n",
       "         8729, 8736, 8737, 8738, 8742, 8769, 8774, 8793, 8794, 8795, 8803,\n",
       "         8810, 8813, 8814, 8826, 8871, 8875, 8883, 8891, 8902, 8916, 8917,\n",
       "         8929, 8932, 8937, 8945, 8984, 9014, 9029, 9035, 9048, 9057, 9066,\n",
       "         9070, 9079, 9085, 9090, 9117, 9118, 9126, 9128, 9134, 9149, 9150,\n",
       "         9173, 9184, 9191, 9202, 9205, 9212, 9218, 9244, 9246, 9254, 9258,\n",
       "         9260, 9270, 9273, 9282, 9285, 9289, 9298, 9321, 9330, 9338, 9345,\n",
       "         9349, 9361, 9370, 9379, 9380, 9383, 9385, 9386, 9388, 9406, 9415,\n",
       "         9416, 9427, 9481, 9501, 9502, 9504, 9505, 9517, 9520, 9521, 9526,\n",
       "         9527, 9538, 9566, 9576, 9581, 9606, 9620, 9638, 9649, 9651, 9661,\n",
       "         9682, 9685, 9689, 9708, 9715, 9719, 9733, 9748, 9755, 9772, 9778,\n",
       "         9787, 9788, 9793, 9798, 9802, 9814, 9840, 9843, 9863, 9873, 9887,\n",
       "         9888, 9890, 9904, 9905, 9911, 9962, 9968])),\n",
       " (array([   0,    1,    3, ..., 9969, 9970, 9971]),\n",
       "  array([   2,    9,   18,   29,   31,   33,   43,   46,   63,   65,   71,\n",
       "           86,  121,  129,  137,  138,  144,  145,  155,  157,  174,  177,\n",
       "          178,  181,  187,  191,  207,  216,  222,  224,  238,  245,  294,\n",
       "          305,  307,  309,  311,  323,  325,  338,  344,  360,  370,  371,\n",
       "          386,  388,  399,  415,  441,  471,  509,  518,  526,  532,  539,\n",
       "          543,  556,  562,  564,  571,  610,  630,  654,  657,  663,  693,\n",
       "          707,  709,  711,  719,  725,  733,  742,  759,  783,  791,  800,\n",
       "          801,  804,  806,  812,  822,  835,  836,  840,  849,  858,  870,\n",
       "          887,  890,  891,  895,  901,  904,  908,  920,  922,  925,  927,\n",
       "          928,  939,  950,  968,  974,  982,  983, 1002, 1022, 1026, 1031,\n",
       "         1049, 1051, 1063, 1082, 1085, 1089, 1108, 1117, 1122, 1136, 1152,\n",
       "         1176, 1179, 1235, 1240, 1244, 1289, 1303, 1318, 1319, 1340, 1344,\n",
       "         1362, 1374, 1375, 1379, 1380, 1400, 1401, 1406, 1431, 1450, 1453,\n",
       "         1476, 1478, 1499, 1509, 1524, 1552, 1570, 1576, 1591, 1603, 1627,\n",
       "         1628, 1651, 1678, 1679, 1693, 1722, 1725, 1749, 1764, 1791, 1793,\n",
       "         1815, 1829, 1835, 1846, 1860, 1869, 1890, 1903, 1927, 1928, 1937,\n",
       "         1946, 1949, 1950, 1961, 1964, 1970, 1972, 2000, 2008, 2012, 2034,\n",
       "         2035, 2036, 2040, 2041, 2044, 2045, 2057, 2062, 2063, 2074, 2081,\n",
       "         2083, 2101, 2102, 2104, 2110, 2115, 2117, 2118, 2121, 2129, 2135,\n",
       "         2143, 2146, 2154, 2182, 2197, 2198, 2203, 2213, 2214, 2240, 2244,\n",
       "         2256, 2259, 2260, 2263, 2272, 2274, 2280, 2295, 2299, 2321, 2332,\n",
       "         2340, 2346, 2362, 2370, 2399, 2402, 2430, 2431, 2453, 2457, 2458,\n",
       "         2465, 2466, 2467, 2472, 2479, 2488, 2491, 2492, 2496, 2520, 2527,\n",
       "         2537, 2544, 2545, 2553, 2563, 2567, 2581, 2582, 2591, 2600, 2606,\n",
       "         2611, 2624, 2631, 2682, 2692, 2709, 2724, 2743, 2817, 2818, 2819,\n",
       "         2843, 2849, 2852, 2862, 2870, 2885, 2894, 2895, 2905, 2908, 2919,\n",
       "         2922, 2925, 2939, 2958, 2964, 2975, 2982, 2984, 3015, 3016, 3017,\n",
       "         3046, 3048, 3069, 3105, 3118, 3122, 3123, 3126, 3129, 3143, 3146,\n",
       "         3149, 3155, 3159, 3163, 3174, 3182, 3191, 3199, 3206, 3208, 3209,\n",
       "         3216, 3221, 3229, 3235, 3242, 3244, 3259, 3268, 3269, 3281, 3283,\n",
       "         3293, 3310, 3352, 3371, 3374, 3393, 3423, 3425, 3427, 3472, 3501,\n",
       "         3502, 3520, 3534, 3537, 3547, 3577, 3580, 3603, 3608, 3616, 3619,\n",
       "         3621, 3626, 3627, 3634, 3640, 3673, 3687, 3689, 3697, 3708, 3713,\n",
       "         3714, 3726, 3749, 3771, 3783, 3804, 3810, 3824, 3857, 3859, 3885,\n",
       "         3898, 3915, 3919, 3920, 3923, 3924, 3930, 3937, 3950, 3958, 3967,\n",
       "         3971, 3972, 3974, 3992, 3997, 4007, 4012, 4015, 4016, 4019, 4022,\n",
       "         4025, 4049, 4070, 4071, 4082, 4085, 4095, 4102, 4110, 4117, 4121,\n",
       "         4143, 4161, 4175, 4176, 4182, 4204, 4211, 4213, 4222, 4240, 4246,\n",
       "         4248, 4260, 4269, 4272, 4297, 4304, 4322, 4331, 4332, 4336, 4344,\n",
       "         4347, 4349, 4357, 4380, 4393, 4394, 4397, 4402, 4403, 4418, 4419,\n",
       "         4421, 4423, 4433, 4445, 4449, 4455, 4459, 4463, 4479, 4522, 4530,\n",
       "         4533, 4534, 4535, 4536, 4559, 4564, 4565, 4585, 4590, 4595, 4610,\n",
       "         4619, 4623, 4628, 4644, 4645, 4666, 4686, 4696, 4708, 4739, 4743,\n",
       "         4744, 4762, 4775, 4777, 4779, 4791, 4799, 4835, 4846, 4848, 4857,\n",
       "         4861, 4903, 4906, 4915, 4918, 4952, 4959, 4977, 4978, 4979, 4987,\n",
       "         4990, 4993, 4995, 4996, 4998, 5031, 5033, 5040, 5055, 5057, 5060,\n",
       "         5067, 5076, 5085, 5114, 5126, 5138, 5163, 5168, 5180, 5187, 5199,\n",
       "         5203, 5214, 5222, 5230, 5244, 5257, 5260, 5261, 5273, 5286, 5293,\n",
       "         5307, 5330, 5338, 5341, 5345, 5351, 5367, 5371, 5388, 5392, 5403,\n",
       "         5405, 5424, 5430, 5441, 5444, 5448, 5468, 5471, 5475, 5481, 5491,\n",
       "         5495, 5504, 5513, 5518, 5528, 5531, 5536, 5537, 5549, 5565, 5569,\n",
       "         5600, 5630, 5634, 5635, 5642, 5656, 5657, 5661, 5663, 5668, 5688,\n",
       "         5692, 5702, 5712, 5720, 5728, 5732, 5737, 5746, 5758, 5762, 5764,\n",
       "         5783, 5788, 5790, 5794, 5797, 5810, 5813, 5825, 5833, 5841, 5854,\n",
       "         5873, 5888, 5899, 5901, 5917, 5924, 5925, 5938, 5978, 5980, 5985,\n",
       "         5986, 5991, 6003, 6008, 6010, 6020, 6031, 6034, 6045, 6051, 6061,\n",
       "         6072, 6076, 6086, 6089, 6094, 6095, 6105, 6116, 6117, 6122, 6123,\n",
       "         6124, 6126, 6141, 6168, 6201, 6204, 6215, 6219, 6220, 6224, 6233,\n",
       "         6235, 6252, 6266, 6269, 6274, 6290, 6313, 6316, 6329, 6333, 6344,\n",
       "         6356, 6376, 6392, 6397, 6401, 6408, 6417, 6448, 6449, 6453, 6457,\n",
       "         6465, 6470, 6474, 6486, 6487, 6491, 6517, 6548, 6573, 6581, 6592,\n",
       "         6594, 6596, 6598, 6600, 6625, 6649, 6651, 6654, 6655, 6656, 6660,\n",
       "         6664, 6671, 6683, 6692, 6695, 6697, 6702, 6727, 6732, 6735, 6755,\n",
       "         6776, 6791, 6792, 6794, 6808, 6824, 6834, 6847, 6849, 6851, 6864,\n",
       "         6865, 6875, 6880, 6888, 6891, 6896, 6911, 6935, 6938, 6954, 6963,\n",
       "         6980, 6981, 6987, 6990, 7006, 7021, 7022, 7023, 7058, 7066, 7086,\n",
       "         7099, 7104, 7130, 7164, 7173, 7180, 7192, 7243, 7258, 7261, 7263,\n",
       "         7270, 7272, 7278, 7281, 7301, 7308, 7318, 7321, 7329, 7339, 7352,\n",
       "         7373, 7378, 7396, 7401, 7402, 7410, 7420, 7428, 7456, 7486, 7493,\n",
       "         7504, 7511, 7512, 7517, 7520, 7521, 7533, 7555, 7568, 7576, 7583,\n",
       "         7584, 7588, 7600, 7619, 7666, 7678, 7703, 7720, 7725, 7727, 7744,\n",
       "         7745, 7756, 7772, 7777, 7789, 7790, 7799, 7806, 7831, 7834, 7837,\n",
       "         7841, 7854, 7856, 7857, 7862, 7864, 7875, 7877, 7891, 7903, 7910,\n",
       "         7912, 7925, 7933, 7956, 7975, 7982, 7996, 7997, 8013, 8028, 8037,\n",
       "         8041, 8049, 8055, 8063, 8067, 8069, 8080, 8083, 8093, 8113, 8116,\n",
       "         8124, 8129, 8131, 8135, 8142, 8162, 8168, 8178, 8183, 8198, 8207,\n",
       "         8219, 8225, 8227, 8232, 8237, 8255, 8263, 8265, 8268, 8269, 8273,\n",
       "         8281, 8282, 8288, 8338, 8346, 8353, 8354, 8370, 8391, 8400, 8401,\n",
       "         8402, 8405, 8414, 8417, 8431, 8433, 8434, 8436, 8451, 8454, 8456,\n",
       "         8459, 8466, 8472, 8496, 8497, 8498, 8503, 8511, 8529, 8532, 8556,\n",
       "         8570, 8583, 8588, 8592, 8595, 8605, 8627, 8629, 8643, 8645, 8672,\n",
       "         8682, 8689, 8692, 8710, 8730, 8735, 8745, 8746, 8755, 8757, 8778,\n",
       "         8792, 8842, 8849, 8850, 8852, 8857, 8864, 8879, 8903, 8924, 8934,\n",
       "         8941, 8950, 8955, 8958, 8965, 8976, 9000, 9001, 9012, 9026, 9034,\n",
       "         9040, 9071, 9072, 9089, 9099, 9137, 9142, 9172, 9211, 9226, 9284,\n",
       "         9296, 9310, 9311, 9312, 9318, 9333, 9347, 9350, 9352, 9368, 9372,\n",
       "         9376, 9396, 9404, 9413, 9417, 9430, 9432, 9453, 9457, 9458, 9460,\n",
       "         9461, 9478, 9480, 9488, 9510, 9532, 9535, 9541, 9550, 9558, 9584,\n",
       "         9595, 9602, 9622, 9628, 9637, 9647, 9659, 9671, 9681, 9702, 9711,\n",
       "         9724, 9740, 9747, 9759, 9764, 9774, 9783, 9790, 9800, 9805, 9825,\n",
       "         9837, 9864, 9906, 9920, 9921, 9951, 9953])),\n",
       " (array([   0,    1,    2, ..., 9969, 9970, 9971]),\n",
       "  array([  14,   15,   20,   42,   52,   69,   74,  111,  115,  123,  130,\n",
       "          131,  156,  165,  169,  203,  204,  205,  210,  212,  213,  215,\n",
       "          234,  243,  273,  295,  303,  312,  313,  329,  331,  356,  372,\n",
       "          377,  379,  387,  393,  394,  433,  437,  438,  439,  451,  462,\n",
       "          465,  470,  472,  477,  483,  491,  510,  514,  521,  527,  535,\n",
       "          549,  557,  565,  579,  602,  622,  625,  651,  661,  666,  669,\n",
       "          677,  684,  686,  697,  708,  712,  718,  728,  737,  750,  756,\n",
       "          764,  775,  781,  788,  792,  798,  813,  863,  866,  871,  874,\n",
       "          878,  892,  897,  898,  903,  929,  933,  938,  944,  946,  949,\n",
       "          957,  963,  965,  967,  975,  988,  999, 1004, 1016, 1027, 1078,\n",
       "         1091, 1093, 1106, 1112, 1127, 1148, 1155, 1166, 1170, 1171, 1193,\n",
       "         1197, 1210, 1230, 1234, 1242, 1247, 1251, 1260, 1262, 1267, 1274,\n",
       "         1311, 1314, 1355, 1363, 1366, 1371, 1372, 1384, 1397, 1405, 1410,\n",
       "         1420, 1428, 1432, 1449, 1483, 1515, 1525, 1532, 1533, 1534, 1538,\n",
       "         1545, 1546, 1551, 1582, 1587, 1607, 1632, 1640, 1642, 1650, 1654,\n",
       "         1672, 1676, 1682, 1687, 1692, 1698, 1699, 1700, 1727, 1739, 1753,\n",
       "         1754, 1760, 1766, 1772, 1782, 1798, 1801, 1816, 1822, 1823, 1855,\n",
       "         1858, 1859, 1863, 1866, 1880, 1884, 1894, 1895, 1898, 1904, 1909,\n",
       "         1929, 1934, 1936, 1938, 1939, 1940, 1943, 1947, 1948, 1958, 1987,\n",
       "         2028, 2039, 2043, 2056, 2093, 2112, 2132, 2137, 2151, 2152, 2187,\n",
       "         2201, 2204, 2208, 2231, 2232, 2238, 2249, 2255, 2265, 2270, 2278,\n",
       "         2279, 2281, 2284, 2290, 2304, 2311, 2327, 2329, 2337, 2338, 2343,\n",
       "         2348, 2369, 2395, 2446, 2460, 2477, 2524, 2525, 2528, 2546, 2552,\n",
       "         2559, 2562, 2576, 2578, 2584, 2607, 2608, 2619, 2621, 2622, 2626,\n",
       "         2634, 2647, 2650, 2652, 2659, 2688, 2703, 2708, 2718, 2729, 2734,\n",
       "         2750, 2753, 2774, 2789, 2797, 2820, 2823, 2830, 2840, 2848, 2851,\n",
       "         2855, 2857, 2859, 2866, 2868, 2889, 2892, 2900, 2903, 2904, 2913,\n",
       "         2937, 2944, 2952, 2955, 2983, 3004, 3013, 3018, 3022, 3036, 3040,\n",
       "         3054, 3062, 3071, 3073, 3096, 3106, 3107, 3109, 3110, 3142, 3154,\n",
       "         3158, 3162, 3169, 3170, 3175, 3196, 3234, 3248, 3272, 3277, 3282,\n",
       "         3296, 3298, 3330, 3344, 3362, 3363, 3385, 3402, 3416, 3424, 3428,\n",
       "         3434, 3437, 3454, 3470, 3485, 3488, 3493, 3499, 3509, 3512, 3513,\n",
       "         3522, 3538, 3541, 3557, 3579, 3586, 3598, 3610, 3617, 3622, 3632,\n",
       "         3652, 3656, 3665, 3666, 3674, 3675, 3681, 3693, 3700, 3777, 3778,\n",
       "         3786, 3794, 3802, 3820, 3835, 3850, 3855, 3856, 3860, 3865, 3880,\n",
       "         3884, 3887, 3903, 3905, 3910, 3934, 3945, 3953, 3978, 4027, 4028,\n",
       "         4040, 4041, 4055, 4056, 4060, 4063, 4074, 4087, 4099, 4116, 4119,\n",
       "         4124, 4134, 4174, 4181, 4192, 4198, 4219, 4233, 4242, 4251, 4256,\n",
       "         4268, 4278, 4285, 4324, 4335, 4340, 4353, 4370, 4378, 4381, 4388,\n",
       "         4396, 4405, 4422, 4428, 4446, 4469, 4472, 4475, 4480, 4493, 4498,\n",
       "         4499, 4500, 4512, 4513, 4518, 4532, 4569, 4607, 4621, 4625, 4641,\n",
       "         4657, 4658, 4661, 4662, 4671, 4678, 4680, 4688, 4695, 4701, 4712,\n",
       "         4715, 4718, 4720, 4722, 4728, 4759, 4778, 4780, 4789, 4803, 4812,\n",
       "         4824, 4825, 4832, 4836, 4839, 4843, 4847, 4851, 4904, 4944, 4945,\n",
       "         4946, 4948, 4951, 4961, 4965, 4991, 4994, 4997, 5000, 5007, 5011,\n",
       "         5013, 5020, 5026, 5028, 5034, 5037, 5043, 5053, 5058, 5059, 5070,\n",
       "         5075, 5103, 5106, 5121, 5122, 5143, 5149, 5169, 5178, 5185, 5195,\n",
       "         5196, 5201, 5219, 5227, 5231, 5236, 5246, 5251, 5255, 5259, 5308,\n",
       "         5311, 5312, 5318, 5323, 5328, 5333, 5342, 5356, 5381, 5384, 5391,\n",
       "         5399, 5406, 5414, 5416, 5421, 5423, 5434, 5435, 5439, 5440, 5442,\n",
       "         5469, 5483, 5486, 5490, 5497, 5499, 5523, 5554, 5567, 5592, 5604,\n",
       "         5609, 5611, 5621, 5625, 5629, 5645, 5660, 5666, 5671, 5687, 5689,\n",
       "         5690, 5708, 5709, 5726, 5739, 5742, 5745, 5748, 5761, 5766, 5777,\n",
       "         5778, 5779, 5784, 5804, 5820, 5828, 5832, 5835, 5850, 5858, 5869,\n",
       "         5872, 5874, 5898, 5919, 5922, 5923, 5937, 5956, 5961, 5962, 5974,\n",
       "         5982, 5989, 6001, 6002, 6005, 6006, 6009, 6012, 6015, 6021, 6028,\n",
       "         6032, 6042, 6043, 6044, 6048, 6056, 6070, 6073, 6082, 6131, 6135,\n",
       "         6164, 6169, 6173, 6185, 6189, 6199, 6227, 6234, 6279, 6287, 6323,\n",
       "         6326, 6334, 6338, 6346, 6353, 6358, 6363, 6373, 6374, 6387, 6388,\n",
       "         6390, 6415, 6442, 6458, 6460, 6471, 6475, 6479, 6498, 6527, 6538,\n",
       "         6542, 6547, 6550, 6610, 6614, 6615, 6631, 6634, 6652, 6686, 6689,\n",
       "         6708, 6715, 6719, 6723, 6743, 6754, 6760, 6769, 6772, 6775, 6780,\n",
       "         6789, 6796, 6815, 6822, 6860, 6866, 6867, 6870, 6873, 6878, 6894,\n",
       "         6907, 6909, 6917, 6931, 6932, 6951, 6965, 6986, 6988, 7000, 7002,\n",
       "         7011, 7015, 7052, 7055, 7064, 7103, 7106, 7119, 7124, 7135, 7144,\n",
       "         7145, 7155, 7158, 7159, 7176, 7183, 7189, 7205, 7209, 7220, 7230,\n",
       "         7235, 7257, 7264, 7286, 7295, 7304, 7316, 7341, 7360, 7364, 7404,\n",
       "         7415, 7416, 7418, 7426, 7437, 7459, 7462, 7472, 7523, 7527, 7529,\n",
       "         7536, 7544, 7550, 7554, 7561, 7596, 7598, 7601, 7603, 7624, 7625,\n",
       "         7626, 7639, 7656, 7668, 7672, 7680, 7685, 7702, 7704, 7733, 7735,\n",
       "         7739, 7742, 7750, 7786, 7792, 7797, 7807, 7820, 7826, 7829, 7840,\n",
       "         7844, 7848, 7850, 7869, 7879, 7882, 7895, 7916, 7924, 7951, 7977,\n",
       "         7979, 7980, 7984, 7987, 8026, 8032, 8033, 8042, 8064, 8072, 8073,\n",
       "         8119, 8120, 8130, 8136, 8155, 8174, 8179, 8188, 8223, 8228, 8231,\n",
       "         8235, 8262, 8297, 8303, 8307, 8322, 8325, 8330, 8332, 8335, 8341,\n",
       "         8360, 8366, 8374, 8383, 8387, 8409, 8435, 8440, 8442, 8443, 8448,\n",
       "         8455, 8469, 8475, 8484, 8493, 8500, 8527, 8539, 8549, 8551, 8562,\n",
       "         8569, 8579, 8582, 8596, 8609, 8610, 8618, 8619, 8625, 8633, 8650,\n",
       "         8651, 8652, 8654, 8656, 8665, 8670, 8675, 8688, 8701, 8702, 8704,\n",
       "         8706, 8712, 8723, 8725, 8739, 8752, 8754, 8766, 8767, 8777, 8800,\n",
       "         8805, 8808, 8812, 8841, 8844, 8846, 8854, 8861, 8865, 8882, 8885,\n",
       "         8887, 8889, 8894, 8904, 8905, 8913, 8933, 8944, 8956, 8962, 8963,\n",
       "         8991, 8996, 9003, 9013, 9023, 9033, 9037, 9043, 9044, 9052, 9062,\n",
       "         9067, 9075, 9087, 9107, 9123, 9136, 9145, 9147, 9157, 9187, 9195,\n",
       "         9227, 9230, 9249, 9250, 9267, 9293, 9306, 9322, 9323, 9329, 9331,\n",
       "         9342, 9365, 9369, 9374, 9378, 9382, 9414, 9419, 9422, 9433, 9441,\n",
       "         9444, 9447, 9455, 9513, 9537, 9545, 9551, 9559, 9562, 9605, 9612,\n",
       "         9615, 9624, 9626, 9633, 9641, 9642, 9670, 9690, 9701, 9710, 9727,\n",
       "         9728, 9743, 9751, 9754, 9762, 9770, 9775, 9785, 9786, 9789, 9794,\n",
       "         9796, 9827, 9829, 9834, 9842, 9856, 9859, 9893, 9894, 9914, 9923,\n",
       "         9933, 9936, 9941, 9946, 9949, 9964]))]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_data_kfold(10,train_X_full,train_y_full) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "es=EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='min', baseline=None, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.6772784509994744"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_full.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_val.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.65032643,  0.63863782,  0.47346548],\n",
       "        [ 0.65668413,  0.65771093,  0.51796939],\n",
       "        [ 0.65668413,  0.65771093,  0.51796939],\n",
       "        ...,\n",
       "        [ 0.4850262 ,  0.5051261 , -0.30853178],\n",
       "        [ 0.4850262 ,  0.5051261 , -0.30853178],\n",
       "        [ 0.40873378,  0.39704518, -0.39118189]],\n",
       "\n",
       "       [[ 0.85377287,  0.83572656,  0.68962733],\n",
       "        [ 0.85377287,  0.84844197,  0.72141583],\n",
       "        [ 0.85377287,  0.84844197,  0.72141583],\n",
       "        ...,\n",
       "        [ 0.5231724 ,  0.54963001, -0.26402787],\n",
       "        [ 0.5231724 ,  0.54963001, -0.26402787],\n",
       "        [ 0.4850262 ,  0.47333759, -0.31488948]],\n",
       "\n",
       "       [[ 0.73297655,  0.73400334,  0.59426181],\n",
       "        [ 0.86013057,  0.85479967,  0.74048894],\n",
       "        [ 0.86013057,  0.85479967,  0.74048894],\n",
       "        ...,\n",
       "        [ 0.54224551,  0.5369146 , -0.28945867],\n",
       "        [ 0.54224551,  0.5369146 , -0.28945867],\n",
       "        [ 0.47231079,  0.42247598, -0.37210879]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-0.34147497, -0.22600955,  0.16193812],\n",
       "        [-0.31604417, -0.22600955,  0.14922272],\n",
       "        [-0.31604417, -0.22600955,  0.14922272],\n",
       "        ...,\n",
       "        [ 0.38330297,  0.63228012,  0.5751887 ],\n",
       "        [ 0.38330297,  0.63228012,  0.5751887 ],\n",
       "        [ 0.38966068,  0.63863782,  0.58154641]],\n",
       "\n",
       "       [[-0.32875957, -0.21329415,  0.10471881],\n",
       "        [-0.27154026, -0.18150564,  0.11743421],\n",
       "        [-0.27154026, -0.18150564,  0.11743421],\n",
       "        ...,\n",
       "        [ 0.5040993 ,  0.65135323,  0.62605031],\n",
       "        [ 0.5040993 ,  0.65135323,  0.62605031],\n",
       "        [ 0.44687999,  0.59413392,  0.568831  ]],\n",
       "\n",
       "       [[-0.27789796, -0.16879024,  0.09836111],\n",
       "        [-0.43684049, -0.34680588, -0.12415844],\n",
       "        [-0.43684049, -0.34680588, -0.12415844],\n",
       "        ...,\n",
       "        [ 0.61853792,  0.63863782,  0.65783882],\n",
       "        [ 0.61853792,  0.63863782,  0.65783882],\n",
       "        [ 0.67575723,  0.68314173,  0.70870043]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_full[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 0\n",
      "Epoch 1/25\n",
      "180/179 [==============================] - 101s 564ms/step - loss: 0.7481 - acc: 0.5389 - val_loss: 0.6390 - val_acc: 0.6407\n",
      "Epoch 2/25\n",
      "180/179 [==============================] - 59s 328ms/step - loss: 0.6657 - acc: 0.6131 - val_loss: 0.6014 - val_acc: 0.7045\n",
      "Epoch 3/25\n",
      "180/179 [==============================] - 61s 339ms/step - loss: 0.6051 - acc: 0.6703 - val_loss: 0.5673 - val_acc: 0.7314\n",
      "Epoch 4/25\n",
      "180/179 [==============================] - 61s 340ms/step - loss: 0.5586 - acc: 0.7087 - val_loss: 0.5387 - val_acc: 0.7549\n",
      "Epoch 5/25\n",
      "180/179 [==============================] - 62s 346ms/step - loss: 0.5194 - acc: 0.7409 - val_loss: 0.5103 - val_acc: 0.7762\n",
      "Epoch 6/25\n",
      "180/179 [==============================] - 62s 345ms/step - loss: 0.4855 - acc: 0.7673 - val_loss: 0.4861 - val_acc: 0.7929\n",
      "Epoch 7/25\n",
      "180/179 [==============================] - 62s 346ms/step - loss: 0.4563 - acc: 0.7868 - val_loss: 0.4631 - val_acc: 0.8064\n",
      "Epoch 8/25\n",
      "180/179 [==============================] - 62s 344ms/step - loss: 0.4321 - acc: 0.8004 - val_loss: 0.4467 - val_acc: 0.8181\n",
      "Epoch 9/25\n",
      "180/179 [==============================] - 62s 347ms/step - loss: 0.4057 - acc: 0.8124 - val_loss: 0.4246 - val_acc: 0.8288\n",
      "Epoch 10/25\n",
      "180/179 [==============================] - 62s 346ms/step - loss: 0.3874 - acc: 0.8290 - val_loss: 0.4076 - val_acc: 0.8422\n",
      "Epoch 11/25\n",
      "180/179 [==============================] - 63s 349ms/step - loss: 0.3686 - acc: 0.8369 - val_loss: 0.3943 - val_acc: 0.8489\n",
      "Epoch 12/25\n",
      "180/179 [==============================] - 63s 351ms/step - loss: 0.3537 - acc: 0.8454 - val_loss: 0.3800 - val_acc: 0.8545\n",
      "Epoch 13/25\n",
      "180/179 [==============================] - 63s 350ms/step - loss: 0.3439 - acc: 0.8492 - val_loss: 0.3683 - val_acc: 0.8612\n",
      "Epoch 14/25\n",
      "180/179 [==============================] - 63s 350ms/step - loss: 0.3290 - acc: 0.8598 - val_loss: 0.3547 - val_acc: 0.8646\n",
      "Epoch 15/25\n",
      "180/179 [==============================] - 63s 349ms/step - loss: 0.3167 - acc: 0.8634 - val_loss: 0.3459 - val_acc: 0.8668\n",
      "Epoch 16/25\n",
      "180/179 [==============================] - 63s 352ms/step - loss: 0.3056 - acc: 0.8711 - val_loss: 0.3348 - val_acc: 0.8719\n",
      "Epoch 17/25\n",
      "180/179 [==============================] - 63s 351ms/step - loss: 0.2999 - acc: 0.8736 - val_loss: 0.3363 - val_acc: 0.8668\n",
      "Epoch 18/25\n",
      "180/179 [==============================] - 64s 355ms/step - loss: 0.2942 - acc: 0.8770 - val_loss: 0.3213 - val_acc: 0.8752\n",
      "Epoch 19/25\n",
      "180/179 [==============================] - 63s 351ms/step - loss: 0.2808 - acc: 0.8836 - val_loss: 0.3187 - val_acc: 0.8769\n",
      "Epoch 20/25\n",
      "180/179 [==============================] - 63s 352ms/step - loss: 0.2740 - acc: 0.8870 - val_loss: 0.3039 - val_acc: 0.8853\n",
      "Epoch 21/25\n",
      "180/179 [==============================] - 63s 350ms/step - loss: 0.2689 - acc: 0.8885 - val_loss: 0.2987 - val_acc: 0.8864\n",
      "Epoch 22/25\n",
      "180/179 [==============================] - 63s 352ms/step - loss: 0.2614 - acc: 0.8910 - val_loss: 0.2930 - val_acc: 0.8903\n",
      "Epoch 23/25\n",
      "180/179 [==============================] - 63s 350ms/step - loss: 0.2573 - acc: 0.8928 - val_loss: 0.2867 - val_acc: 0.8931\n",
      "Epoch 24/25\n",
      "180/179 [==============================] - 63s 352ms/step - loss: 0.2473 - acc: 0.9017 - val_loss: 0.2774 - val_acc: 0.8965\n",
      "Epoch 25/25\n",
      "180/179 [==============================] - 63s 350ms/step - loss: 0.2486 - acc: 0.8993 - val_loss: 0.2792 - val_acc: 0.8982\n",
      "1787/1787 [==============================] - 5s 3ms/step\n",
      "[0.27924036822074877, 0.898153329369204]\n",
      "\n",
      "Fold 1\n",
      "Epoch 1/25\n",
      "180/179 [==============================] - 67s 370ms/step - loss: 0.2402 - acc: 0.9015 - val_loss: 0.2793 - val_acc: 0.8959\n",
      "Epoch 2/25\n",
      "180/179 [==============================] - 61s 340ms/step - loss: 0.2331 - acc: 0.9058 - val_loss: 0.2739 - val_acc: 0.8998\n",
      "Epoch 3/25\n",
      "180/179 [==============================] - 63s 350ms/step - loss: 0.2279 - acc: 0.9076 - val_loss: 0.2736 - val_acc: 0.8982\n",
      "Epoch 4/25\n",
      "180/179 [==============================] - 62s 347ms/step - loss: 0.2255 - acc: 0.9090 - val_loss: 0.2705 - val_acc: 0.9010\n",
      "Epoch 5/25\n",
      "180/179 [==============================] - 65s 359ms/step - loss: 0.2233 - acc: 0.9103 - val_loss: 0.2645 - val_acc: 0.9060\n",
      "Epoch 6/25\n",
      "180/179 [==============================] - 65s 361ms/step - loss: 0.2161 - acc: 0.9133 - val_loss: 0.2648 - val_acc: 0.9054\n",
      "Epoch 7/25\n",
      "180/179 [==============================] - 65s 361ms/step - loss: 0.2158 - acc: 0.9131 - val_loss: 0.2589 - val_acc: 0.9071\n",
      "Epoch 8/25\n",
      "180/179 [==============================] - 65s 361ms/step - loss: 0.2098 - acc: 0.9185 - val_loss: 0.2575 - val_acc: 0.9077\n",
      "Epoch 9/25\n",
      "180/179 [==============================] - 65s 362ms/step - loss: 0.2092 - acc: 0.9176 - val_loss: 0.2520 - val_acc: 0.9099\n",
      "Epoch 10/25\n",
      "180/179 [==============================] - 65s 363ms/step - loss: 0.2019 - acc: 0.9208 - val_loss: 0.2506 - val_acc: 0.9105\n",
      "Epoch 11/25\n",
      "180/179 [==============================] - 65s 362ms/step - loss: 0.1964 - acc: 0.9236 - val_loss: 0.2468 - val_acc: 0.9116\n",
      "Epoch 12/25\n",
      "180/179 [==============================] - 65s 359ms/step - loss: 0.1957 - acc: 0.9248 - val_loss: 0.2584 - val_acc: 0.9077\n",
      "Epoch 13/25\n",
      "180/179 [==============================] - 65s 362ms/step - loss: 0.1933 - acc: 0.9250 - val_loss: 0.2536 - val_acc: 0.9065\n",
      "Epoch 14/25\n",
      "180/179 [==============================] - 63s 350ms/step - loss: 0.1895 - acc: 0.9274 - val_loss: 0.2456 - val_acc: 0.9138\n",
      "Epoch 15/25\n",
      "180/179 [==============================] - 63s 350ms/step - loss: 0.1858 - acc: 0.9280 - val_loss: 0.2426 - val_acc: 0.9133\n",
      "Epoch 16/25\n",
      "180/179 [==============================] - 63s 352ms/step - loss: 0.1837 - acc: 0.9291 - val_loss: 0.2433 - val_acc: 0.9116\n",
      "Epoch 17/25\n",
      "180/179 [==============================] - 63s 349ms/step - loss: 0.1796 - acc: 0.9310 - val_loss: 0.2432 - val_acc: 0.9116\n",
      "Epoch 18/25\n",
      "180/179 [==============================] - 63s 351ms/step - loss: 0.1813 - acc: 0.9302 - val_loss: 0.2440 - val_acc: 0.9105\n",
      "Epoch 19/25\n",
      "180/179 [==============================] - 63s 352ms/step - loss: 0.1760 - acc: 0.9330 - val_loss: 0.2392 - val_acc: 0.9116\n",
      "Epoch 20/25\n",
      "180/179 [==============================] - 63s 353ms/step - loss: 0.1770 - acc: 0.9333 - val_loss: 0.2443 - val_acc: 0.9077\n",
      "Epoch 21/25\n",
      "180/179 [==============================] - 63s 351ms/step - loss: 0.1717 - acc: 0.9347 - val_loss: 0.2361 - val_acc: 0.9133\n",
      "Epoch 22/25\n",
      "180/179 [==============================] - 64s 358ms/step - loss: 0.1684 - acc: 0.9365 - val_loss: 0.2431 - val_acc: 0.9099\n",
      "Epoch 23/25\n",
      "180/179 [==============================] - 115s 638ms/step - loss: 0.1662 - acc: 0.9362 - val_loss: 0.2351 - val_acc: 0.9138\n",
      "Epoch 24/25\n",
      "180/179 [==============================] - 188s 1s/step - loss: 0.1651 - acc: 0.9371 - val_loss: 0.2376 - val_acc: 0.9149\n",
      "Epoch 25/25\n",
      "180/179 [==============================] - 190s 1s/step - loss: 0.1690 - acc: 0.9356 - val_loss: 0.2419 - val_acc: 0.9099\n",
      "1787/1787 [==============================] - 12s 6ms/step\n",
      "[0.24187360501316052, 0.9099048689282935]\n",
      "\n",
      "Fold 2\n",
      "Epoch 1/25\n",
      "180/179 [==============================] - 209s 1s/step - loss: 0.1699 - acc: 0.9353 - val_loss: 0.2341 - val_acc: 0.9166\n",
      "Epoch 2/25\n",
      "180/179 [==============================] - 185s 1s/step - loss: 0.1637 - acc: 0.9370 - val_loss: 0.2350 - val_acc: 0.9149\n",
      "Epoch 3/25\n",
      "180/179 [==============================] - 190s 1s/step - loss: 0.1607 - acc: 0.9395 - val_loss: 0.2271 - val_acc: 0.9183\n",
      "Epoch 4/25\n",
      " 22/179 [==>...........................] - ETA: 2:46 - loss: 0.1570 - acc: 0.9393"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-9e0b5e1d3648>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m                         \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain_X_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m                         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m                     )\n\u001b[0;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m                 \u001b[0mgenerator_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__len__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    683\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    684\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m                 \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 651\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    652\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mready\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    653\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 648\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    649\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    553\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "gen = ImageDataGenerator(horizontal_flip = True,\n",
    "                         vertical_flip = False,\n",
    "                         width_shift_range = 0.1,\n",
    "                         height_shift_range = 0.1,\n",
    "                         zoom_range = 0.1,\n",
    "                         rotation_range = 10\n",
    "                        )\n",
    "\n",
    "\n",
    "for j, (train_idx, val_idx) in enumerate(folds):\n",
    "    print('\\nFold', j)\n",
    "    #print(np.array(y_label)[train_idx])\n",
    "    X_train_cv = train_X_full[train_idx]\n",
    "    y_train_cv = train_y_full[train_idx]\n",
    "    X_valid_cv = train_X_full[val_idx]\n",
    "    y_valid_cv= train_y_full[val_idx]\n",
    "    #gen.fit(X_train_cv)\n",
    "    \n",
    "#        name_weights = \"final_model_fold\" + str(j) + \"_weights.h5\"\n",
    "#        callbacks = get_callbacks(name_weights = name_weights, patience_lr=10)\n",
    "    generator = gen.flow(X_train_cv, y_train_cv, batch_size = 128)\n",
    "#        model = get_model()\n",
    "    model.fit_generator(generator,\n",
    "                        steps_per_epoch=len(X_train_cv)/50,\n",
    "                        epochs=25,\n",
    "                        shuffle=False,\n",
    "                        verbose=1,\n",
    "                        \n",
    "                        validation_data = (train_X_val, train_y_val),\n",
    "                        validation_steps=len(train_X_val),\n",
    "                        callbacks=[es]         \n",
    "                    )\n",
    "    print(model.evaluate(train_X_val, train_y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_mobile_net_layer_train_false.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting with k fold for PL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#starting with k fold on 100 images and validation for pseudo labelling\n",
    "x_1,inps_1=model_design()\n",
    "model_k_fold=Model(inputs=inps_1,outputs=x_1)\n",
    "model_k_fold.compile(Adam(lr=1e-6), loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import preprocess_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = preprocess_input(train_X)\n",
    "train_X_val= preprocess_input(train_X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Load_data_kfold\n",
      "Exit Load_data_kfold\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(array([  0,   1,   4,   5,   6,   7,   8,   9,  11,  12,  13,  14,  15,\n",
       "          16,  17,  18,  20,  22,  23,  25,  28,  29,  30,  31,  32,  34,\n",
       "          37,  38,  42,  43,  45,  47,  49,  51,  54,  55,  56,  57,  58,\n",
       "          59,  60,  61,  63,  64,  67,  68,  69,  70,  73,  74,  75,  76,\n",
       "          77,  78,  79,  82,  83,  85,  86,  87,  91,  92,  93,  94,  96,\n",
       "          98,  99, 100, 101, 104, 105, 107, 108, 109, 110, 111, 113, 114,\n",
       "         117, 118]),\n",
       "  array([  2,   3,  10,  19,  21,  24,  26,  27,  33,  35,  36,  39,  40,\n",
       "          41,  44,  46,  48,  50,  52,  53,  62,  65,  66,  71,  72,  80,\n",
       "          81,  84,  88,  89,  90,  95,  97, 102, 103, 106, 112, 115, 116,\n",
       "         119])),\n",
       " (array([  0,   1,   2,   3,   5,   6,   7,   8,   9,  10,  11,  12,  15,\n",
       "          16,  18,  19,  20,  21,  24,  25,  26,  27,  33,  35,  36,  37,\n",
       "          39,  40,  41,  43,  44,  45,  46,  48,  50,  52,  53,  57,  58,\n",
       "          59,  60,  62,  63,  64,  65,  66,  67,  69,  71,  72,  75,  76,\n",
       "          78,  80,  81,  82,  83,  84,  85,  86,  88,  89,  90,  94,  95,\n",
       "          97,  99, 100, 101, 102, 103, 105, 106, 107, 111, 112, 115, 116,\n",
       "         117, 119]),\n",
       "  array([  4,  13,  14,  17,  22,  23,  28,  29,  30,  31,  32,  34,  38,\n",
       "          42,  47,  49,  51,  54,  55,  56,  61,  68,  70,  73,  74,  77,\n",
       "          79,  87,  91,  92,  93,  96,  98, 104, 108, 109, 110, 113, 114,\n",
       "         118])),\n",
       " (array([  2,   3,   4,  10,  13,  14,  17,  19,  21,  22,  23,  24,  26,\n",
       "          27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  38,  39,  40,\n",
       "          41,  42,  44,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
       "          56,  61,  62,  65,  66,  68,  70,  71,  72,  73,  74,  77,  79,\n",
       "          80,  81,  84,  87,  88,  89,  90,  91,  92,  93,  95,  96,  97,\n",
       "          98, 102, 103, 104, 106, 108, 109, 110, 112, 113, 114, 115, 116,\n",
       "         118, 119]),\n",
       "  array([  0,   1,   5,   6,   7,   8,   9,  11,  12,  15,  16,  18,  20,\n",
       "          25,  37,  43,  45,  57,  58,  59,  60,  63,  64,  67,  69,  75,\n",
       "          76,  78,  82,  83,  85,  86,  94,  99, 100, 101, 105, 107, 111,\n",
       "         117]))]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_data_kfold(3,train_X,train_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151.061"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_val.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = to_categorical(train_y, 2)\n",
    "train_y_val =to_categorical(train_y_val, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 0\n",
      "Epoch 1/25\n",
      "120/120 [==============================] - 33s 273ms/step - loss: 4.9345 - acc: 0.5690 - val_loss: 5.8338 - val_acc: 0.4522\n",
      "Epoch 2/25\n",
      "120/120 [==============================] - 27s 226ms/step - loss: 3.8276 - acc: 0.6457 - val_loss: 5.0691 - val_acc: 0.5104\n",
      "Epoch 3/25\n",
      "120/120 [==============================] - 27s 226ms/step - loss: 3.1880 - acc: 0.6945 - val_loss: 4.4468 - val_acc: 0.5618\n",
      "Epoch 4/25\n",
      "120/120 [==============================] - 28s 230ms/step - loss: 2.4726 - acc: 0.7544 - val_loss: 4.0450 - val_acc: 0.5921\n",
      "Epoch 5/25\n",
      "120/120 [==============================] - 28s 230ms/step - loss: 2.0196 - acc: 0.7878 - val_loss: 3.6006 - val_acc: 0.6346\n",
      "Epoch 6/25\n",
      "120/120 [==============================] - 28s 232ms/step - loss: 1.6397 - acc: 0.8251 - val_loss: 3.4117 - val_acc: 0.6570\n",
      "Epoch 7/25\n",
      "120/120 [==============================] - 28s 232ms/step - loss: 1.3505 - acc: 0.8478 - val_loss: 3.1448 - val_acc: 0.6844\n",
      "Epoch 8/25\n",
      "120/120 [==============================] - 28s 235ms/step - loss: 1.1265 - acc: 0.8734 - val_loss: 3.0203 - val_acc: 0.6984\n",
      "Epoch 9/25\n",
      "120/120 [==============================] - 28s 233ms/step - loss: 1.0545 - acc: 0.8798 - val_loss: 2.9383 - val_acc: 0.7084\n",
      "Epoch 10/25\n",
      "120/120 [==============================] - 28s 234ms/step - loss: 0.8613 - acc: 0.8998 - val_loss: 2.8627 - val_acc: 0.7202\n",
      "Epoch 11/25\n",
      "120/120 [==============================] - 28s 234ms/step - loss: 0.6903 - acc: 0.9139 - val_loss: 2.7566 - val_acc: 0.7331\n",
      "Epoch 12/25\n",
      "120/120 [==============================] - 28s 236ms/step - loss: 0.6387 - acc: 0.9216 - val_loss: 2.7261 - val_acc: 0.7409\n",
      "Epoch 13/25\n",
      "120/120 [==============================] - 29s 245ms/step - loss: 0.5571 - acc: 0.9328 - val_loss: 2.7080 - val_acc: 0.7465\n",
      "Epoch 14/25\n",
      "120/120 [==============================] - 29s 239ms/step - loss: 0.5130 - acc: 0.9380 - val_loss: 2.6887 - val_acc: 0.7504\n",
      "Epoch 15/25\n",
      "120/120 [==============================] - 28s 236ms/step - loss: 0.4143 - acc: 0.9455 - val_loss: 2.6718 - val_acc: 0.7538\n",
      "Epoch 16/25\n",
      "120/120 [==============================] - 28s 236ms/step - loss: 0.3754 - acc: 0.9510 - val_loss: 2.6412 - val_acc: 0.7577\n",
      "Epoch 17/25\n",
      "120/120 [==============================] - 28s 237ms/step - loss: 0.3222 - acc: 0.9565 - val_loss: 2.6314 - val_acc: 0.7588\n",
      "Epoch 18/25\n",
      "120/120 [==============================] - 28s 237ms/step - loss: 0.3196 - acc: 0.9583 - val_loss: 2.6173 - val_acc: 0.7616\n",
      "Epoch 19/25\n",
      "120/120 [==============================] - 28s 236ms/step - loss: 0.2866 - acc: 0.9620 - val_loss: 2.6151 - val_acc: 0.7639\n",
      "Epoch 20/25\n",
      "120/120 [==============================] - 28s 236ms/step - loss: 0.2328 - acc: 0.9698 - val_loss: 2.5845 - val_acc: 0.7650\n",
      "Epoch 21/25\n",
      "120/120 [==============================] - 28s 236ms/step - loss: 0.2225 - acc: 0.9667 - val_loss: 2.5580 - val_acc: 0.7683\n",
      "Epoch 22/25\n",
      "120/120 [==============================] - 28s 236ms/step - loss: 0.2048 - acc: 0.9723 - val_loss: 2.5330 - val_acc: 0.7683\n",
      "Epoch 23/25\n",
      "120/120 [==============================] - 28s 237ms/step - loss: 0.1878 - acc: 0.9717 - val_loss: 2.5117 - val_acc: 0.7722\n",
      "Epoch 24/25\n",
      "120/120 [==============================] - 28s 237ms/step - loss: 0.1862 - acc: 0.9735 - val_loss: 2.4844 - val_acc: 0.7728\n",
      "Epoch 25/25\n",
      "120/120 [==============================] - 28s 236ms/step - loss: 0.1684 - acc: 0.9748 - val_loss: 2.4922 - val_acc: 0.7706\n",
      "1787/1787 [==============================] - 3s 2ms/step: \n",
      "[2.49222055319582, 0.7705651934946057]\n",
      "\n",
      "Fold 1\n",
      "Epoch 1/25\n",
      "120/120 [==============================] - 31s 257ms/step - loss: 1.1643 - acc: 0.8765 - val_loss: 2.0641 - val_acc: 0.8092\n",
      "Epoch 2/25\n",
      "120/120 [==============================] - 28s 237ms/step - loss: 0.6956 - acc: 0.9220 - val_loss: 1.8940 - val_acc: 0.8248\n",
      "Epoch 3/25\n",
      "120/120 [==============================] - 28s 237ms/step - loss: 0.5186 - acc: 0.9359 - val_loss: 1.7760 - val_acc: 0.8366\n",
      "Epoch 4/25\n",
      "120/120 [==============================] - 28s 237ms/step - loss: 0.4181 - acc: 0.9458 - val_loss: 1.7415 - val_acc: 0.8411\n",
      "Epoch 5/25\n",
      "120/120 [==============================] - 28s 235ms/step - loss: 0.3415 - acc: 0.9553 - val_loss: 1.7390 - val_acc: 0.8383\n",
      "Epoch 6/25\n",
      "120/120 [==============================] - 28s 237ms/step - loss: 0.2856 - acc: 0.9633 - val_loss: 1.6889 - val_acc: 0.8450\n",
      "Epoch 7/25\n",
      "120/120 [==============================] - 28s 237ms/step - loss: 0.2307 - acc: 0.9699 - val_loss: 1.6890 - val_acc: 0.8461\n",
      "Epoch 8/25\n",
      "120/120 [==============================] - 28s 235ms/step - loss: 0.1985 - acc: 0.9738 - val_loss: 1.7045 - val_acc: 0.8450\n",
      "Epoch 9/25\n",
      "120/120 [==============================] - 28s 235ms/step - loss: 0.1904 - acc: 0.9750 - val_loss: 1.6826 - val_acc: 0.8461\n",
      "Epoch 10/25\n",
      "120/120 [==============================] - 28s 237ms/step - loss: 0.1898 - acc: 0.9760 - val_loss: 1.6949 - val_acc: 0.8456\n",
      "Epoch 11/25\n",
      "120/120 [==============================] - 28s 237ms/step - loss: 0.1618 - acc: 0.9786 - val_loss: 1.6869 - val_acc: 0.8461\n",
      "Epoch 12/25\n",
      "120/120 [==============================] - 29s 238ms/step - loss: 0.1349 - acc: 0.9808 - val_loss: 1.6474 - val_acc: 0.8489\n",
      "Epoch 13/25\n",
      "120/120 [==============================] - 29s 238ms/step - loss: 0.1315 - acc: 0.9808 - val_loss: 1.6637 - val_acc: 0.8483\n",
      "Epoch 14/25\n",
      "120/120 [==============================] - 29s 240ms/step - loss: 0.1133 - acc: 0.9845 - val_loss: 1.6608 - val_acc: 0.8478\n",
      "Epoch 15/25\n",
      "120/120 [==============================] - 29s 239ms/step - loss: 0.0937 - acc: 0.9860 - val_loss: 1.6551 - val_acc: 0.8478\n",
      "Epoch 16/25\n",
      "120/120 [==============================] - 28s 237ms/step - loss: 0.0992 - acc: 0.9859 - val_loss: 1.6131 - val_acc: 0.8506\n",
      "Epoch 17/25\n",
      "120/120 [==============================] - 28s 236ms/step - loss: 0.0836 - acc: 0.9867 - val_loss: 1.6276 - val_acc: 0.8511\n",
      "Epoch 18/25\n",
      "120/120 [==============================] - 29s 239ms/step - loss: 0.0780 - acc: 0.9891 - val_loss: 1.6335 - val_acc: 0.8506\n",
      "Epoch 19/25\n",
      "120/120 [==============================] - 29s 239ms/step - loss: 0.0802 - acc: 0.9883 - val_loss: 1.6239 - val_acc: 0.8500\n",
      "Epoch 20/25\n",
      "120/120 [==============================] - 29s 238ms/step - loss: 0.0696 - acc: 0.9892 - val_loss: 1.6362 - val_acc: 0.8511\n",
      "Epoch 21/25\n",
      "120/120 [==============================] - 29s 242ms/step - loss: 0.0684 - acc: 0.9893 - val_loss: 1.6514 - val_acc: 0.8500\n",
      "Epoch 22/25\n",
      "120/120 [==============================] - 29s 238ms/step - loss: 0.0620 - acc: 0.9908 - val_loss: 1.6582 - val_acc: 0.8489\n",
      "Epoch 23/25\n",
      "120/120 [==============================] - 29s 238ms/step - loss: 0.0634 - acc: 0.9915 - val_loss: 1.6388 - val_acc: 0.8511\n",
      "Epoch 24/25\n",
      "120/120 [==============================] - 29s 238ms/step - loss: 0.0563 - acc: 0.9907 - val_loss: 1.6169 - val_acc: 0.8517\n",
      "Epoch 25/25\n",
      "120/120 [==============================] - 29s 238ms/step - loss: 0.0612 - acc: 0.9906 - val_loss: 1.6203 - val_acc: 0.8511\n",
      "1787/1787 [==============================] - 3s 2ms/step\n",
      "[1.6203345079294886, 0.851147174034695]\n",
      "\n",
      "Fold 2\n",
      "Epoch 1/25\n",
      "120/120 [==============================] - 30s 252ms/step - loss: 0.1046 - acc: 0.9861 - val_loss: 1.6320 - val_acc: 0.8523\n",
      "Epoch 2/25\n",
      "120/120 [==============================] - 28s 231ms/step - loss: 0.0822 - acc: 0.9892 - val_loss: 1.5991 - val_acc: 0.8523\n",
      "Epoch 3/25\n",
      "120/120 [==============================] - 28s 229ms/step - loss: 0.0851 - acc: 0.9882 - val_loss: 1.6098 - val_acc: 0.8528\n",
      "Epoch 4/25\n",
      "120/120 [==============================] - 28s 231ms/step - loss: 0.0531 - acc: 0.9929 - val_loss: 1.5900 - val_acc: 0.8523\n",
      "Epoch 5/25\n",
      "120/120 [==============================] - 28s 230ms/step - loss: 0.0527 - acc: 0.9914 - val_loss: 1.5822 - val_acc: 0.8551\n",
      "Epoch 6/25\n",
      "120/120 [==============================] - 28s 229ms/step - loss: 0.0634 - acc: 0.9915 - val_loss: 1.5604 - val_acc: 0.8579\n",
      "Epoch 7/25\n",
      "120/120 [==============================] - 27s 229ms/step - loss: 0.0479 - acc: 0.9928 - val_loss: 1.5503 - val_acc: 0.8573\n",
      "Epoch 8/25\n",
      "120/120 [==============================] - 27s 229ms/step - loss: 0.0558 - acc: 0.9919 - val_loss: 1.5405 - val_acc: 0.8584\n",
      "Epoch 9/25\n",
      "120/120 [==============================] - 28s 230ms/step - loss: 0.0548 - acc: 0.9922 - val_loss: 1.5303 - val_acc: 0.8579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/25\n",
      "120/120 [==============================] - 27s 229ms/step - loss: 0.0345 - acc: 0.9946 - val_loss: 1.5308 - val_acc: 0.8595\n",
      "Epoch 11/25\n",
      "120/120 [==============================] - 28s 230ms/step - loss: 0.0367 - acc: 0.9940 - val_loss: 1.5315 - val_acc: 0.8595\n",
      "Epoch 12/25\n",
      "120/120 [==============================] - 28s 232ms/step - loss: 0.0343 - acc: 0.9945 - val_loss: 1.5392 - val_acc: 0.8567\n",
      "Epoch 13/25\n",
      "120/120 [==============================] - 28s 231ms/step - loss: 0.0321 - acc: 0.9944 - val_loss: 1.5177 - val_acc: 0.8590\n",
      "Epoch 14/25\n",
      "120/120 [==============================] - 27s 229ms/step - loss: 0.0303 - acc: 0.9951 - val_loss: 1.4903 - val_acc: 0.8595\n",
      "Epoch 15/25\n",
      "120/120 [==============================] - 27s 228ms/step - loss: 0.0312 - acc: 0.9955 - val_loss: 1.4867 - val_acc: 0.8584\n",
      "Epoch 16/25\n",
      "120/120 [==============================] - 27s 229ms/step - loss: 0.0263 - acc: 0.9954 - val_loss: 1.5047 - val_acc: 0.8590\n",
      "Epoch 17/25\n",
      "120/120 [==============================] - 27s 229ms/step - loss: 0.0281 - acc: 0.9956 - val_loss: 1.5208 - val_acc: 0.8562\n",
      "Epoch 18/25\n",
      "120/120 [==============================] - 28s 233ms/step - loss: 0.0264 - acc: 0.9952 - val_loss: 1.4564 - val_acc: 0.8640\n",
      "Epoch 19/25\n",
      "120/120 [==============================] - 27s 228ms/step - loss: 0.0160 - acc: 0.9971 - val_loss: 1.4795 - val_acc: 0.8612\n",
      "Epoch 20/25\n",
      "120/120 [==============================] - 28s 230ms/step - loss: 0.0134 - acc: 0.9972 - val_loss: 1.4663 - val_acc: 0.8612\n",
      "Epoch 21/25\n",
      "120/120 [==============================] - 28s 231ms/step - loss: 0.0181 - acc: 0.9974 - val_loss: 1.4795 - val_acc: 0.8607\n",
      "Epoch 22/25\n",
      "120/120 [==============================] - 28s 230ms/step - loss: 0.0201 - acc: 0.9969 - val_loss: 1.4678 - val_acc: 0.8612\n",
      "Epoch 23/25\n",
      "120/120 [==============================] - 28s 229ms/step - loss: 0.0180 - acc: 0.9969 - val_loss: 1.4752 - val_acc: 0.8623\n",
      "Epoch 24/25\n",
      "120/120 [==============================] - 28s 229ms/step - loss: 0.0180 - acc: 0.9974 - val_loss: 1.4789 - val_acc: 0.8629\n",
      "Epoch 25/25\n",
      "120/120 [==============================] - 28s 230ms/step - loss: 0.0184 - acc: 0.9965 - val_loss: 1.4385 - val_acc: 0.8663\n",
      "1787/1787 [==============================] - 3s 2ms/step\n",
      "[1.4384707042654845, 0.8662562954672636]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gen = ImageDataGenerator(horizontal_flip = True,\n",
    "                         vertical_flip = False,\n",
    "                         width_shift_range = 0.1,\n",
    "                         height_shift_range = 0.1,\n",
    "                         zoom_range = 0.1,\n",
    "                         rotation_range = 10\n",
    "                        )\n",
    "\n",
    "\n",
    "for j, (train_idx, val_idx) in enumerate(folds):\n",
    "    print('\\nFold', j)\n",
    "    #print(np.array(y_label)[train_idx])\n",
    "    X_train_cv = train_X[train_idx]\n",
    "    y_train_cv = train_y[train_idx]\n",
    "    X_valid_cv = train_X[val_idx]\n",
    "    y_valid_cv= train_y[val_idx]\n",
    "    #gen.fit(X_train_cv)\n",
    "    \n",
    "#        name_weights = \"final_model_fold\" + str(j) + \"_weights.h5\"\n",
    "#        callbacks = get_callbacks(name_weights = name_weights, patience_lr=10)\n",
    "    generator = gen.flow(X_train_cv, y_train_cv, batch_size = 128)\n",
    "#        model = get_model()\n",
    "    model_k_fold.fit_generator(generator,\n",
    "                        steps_per_epoch=len(train_X),\n",
    "                        epochs=25,\n",
    "                        shuffle=False,\n",
    "                        verbose=1,\n",
    "                        \n",
    "                        validation_data = (train_X_val, train_y_val),\n",
    "                        validation_steps=len(train_X_val),\n",
    "                        callbacks=[es]         \n",
    "                    )\n",
    "    print(model_k_fold.evaluate(train_X_val, train_y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_k_fold.save(\"NewestModel_pre_PL.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.convolutional.Conv2D at 0x129cb27ccc0>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_k_fold.layers[-8] #0x129cb27ccc0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_temp=Model(input=model_k_fold.inputs,output=model_k_fold.layers[-8].output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_temp.compile(Adam(lr=1e-6), loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "model_7 (Model)              (None, 8, 8, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 512)               16777728  \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 31,756,098\n",
      "Trainable params: 19,401,218\n",
      "Non-trainable params: 12,354,880\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_v2=Sequential()\n",
    "model_v2.add(model_temp)\n",
    "model_v2.add(layers.Flatten())\n",
    "model_v2.add(Dropout(0.35))\n",
    "model_v2.add(layers.Dense(512, activation='relu'))\n",
    "model_v2.add(Dropout(0.25))\n",
    "model_v2.add(layers.Dense(512, activation='relu'))\n",
    "model_v2.add(layers.Dropout(0.4))\n",
    "model_v2.add(layers.Dense(2, activation='softmax'))\n",
    "model_v2.summary()\n",
    "model_v2.compile(Adam(lr=1e-6), 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 0\n",
      "Epoch 1/25\n",
      " 31/120 [======>.......................] - ETA: 27s - loss: 5.2315 - acc: 0.4798"
     ]
    }
   ],
   "source": [
    "\n",
    "gen = ImageDataGenerator(horizontal_flip = True,\n",
    "                         vertical_flip = False,\n",
    "                         width_shift_range = 0.1,\n",
    "                         height_shift_range = 0.1,\n",
    "                         zoom_range = 0.1,\n",
    "                         rotation_range = 10\n",
    "                        )\n",
    "\n",
    "\n",
    "for j, (train_idx, val_idx) in enumerate(folds):\n",
    "    print('\\nFold', j)\n",
    "    #print(np.array(y_label)[train_idx])\n",
    "    X_train_cv = train_X[train_idx]\n",
    "    y_train_cv = train_y[train_idx]\n",
    "    X_valid_cv = train_X[val_idx]\n",
    "    y_valid_cv= train_y[val_idx]\n",
    "    #gen.fit(X_train_cv)\n",
    "    \n",
    "#        name_weights = \"final_model_fold\" + str(j) + \"_weights.h5\"\n",
    "#        callbacks = get_callbacks(name_weights = name_weights, patience_lr=10)\n",
    "    generator = gen.flow(X_train_cv, y_train_cv, batch_size = 128)\n",
    "#        model = get_model()\n",
    "    model_v2.fit_generator(generator,\n",
    "                        steps_per_epoch=len(train_X),\n",
    "                        epochs=25,\n",
    "                        shuffle=False,\n",
    "                        verbose=1,\n",
    "                        \n",
    "                        validation_data = (train_X_val, train_y_val),\n",
    "                        validation_steps=len(train_X_val),\n",
    "                        callbacks=[es]         \n",
    "                    )\n",
    "    print(model_v2.evaluate(train_X_val, train_y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 0\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_18 to have 4 dimensions, but got array with shape (1787, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-107-2a8baec3d073>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m                         \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m                         \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain_X_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m                         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m                     )\n\u001b[0;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_v2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    142\u001b[0m                                      str(validation_data))\n\u001b[0;32m    143\u001b[0m                 val_x, val_y, val_sample_weights = model._standardize_user_data(\n\u001b[1;32m--> 144\u001b[1;33m                     val_x, val_y, val_sample_weight)\n\u001b[0m\u001b[0;32m    145\u001b[0m                 \u001b[0mval_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_x\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mval_y\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mval_sample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m                 if model.uses_learning_phase and not isinstance(K.learning_phase(),\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m                 exception_prefix='target')\n\u001b[0m\u001b[0;32m    790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    126\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    129\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected dense_18 to have 4 dimensions, but got array with shape (1787, 2)"
     ]
    }
   ],
   "source": [
    "\n",
    "gen = ImageDataGenerator(horizontal_flip = True,\n",
    "                         vertical_flip = False,\n",
    "                         width_shift_range = 0.1,\n",
    "                         height_shift_range = 0.1,\n",
    "                         zoom_range = 0.1,\n",
    "                         rotation_range = 10\n",
    "                        )\n",
    "\n",
    "\n",
    "\n",
    "for j, (train_idx, val_idx) in enumerate(folds):\n",
    "    print('\\nFold', j)\n",
    "    #print(np.array(y_label)[train_idx])\n",
    "    X_train_cv = train_X[train_idx]\n",
    "    y_train_cv = train_y[train_idx]\n",
    "    X_valid_cv = train_X[val_idx]\n",
    "    y_valid_cv= train_y[val_idx]\n",
    "    #gen.fit(X_train_cv)\n",
    "    \n",
    "#        name_weights = \"final_model_fold\" + str(j) + \"_weights.h5\"\n",
    "#        callbacks = get_callbacks(name_weights = name_weights, patience_lr=10)\n",
    "    generator = gen.flow(X_train_cv, y_train_cv, batch_size = 128)\n",
    "#        model = get_model()\n",
    "    model_v2.fit_generator(generator,\n",
    "                        steps_per_epoch=64,\n",
    "                        epochs=25,\n",
    "                        shuffle=True,\n",
    "                        verbose=1,\n",
    "                        validation_data = (train_X_val, train_y_val),\n",
    "                        callbacks=[es]         \n",
    "                    )\n",
    "    print(model_v2.evaluate(train_X_val, train_y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unlabeled_data_processing(unlb_data, batch_size):\n",
    "#    unlb_data = \"/Users/tushar/Desktop/data_after_cluster/unlab\"\n",
    "#    batch_size = 128\n",
    "    print(\"Enter Unlab_data_process\")\n",
    "#    os.mkdir('dummy')\n",
    "    gen =ImageDataGenerator()\n",
    "    data = gen.flow_from_directory(unlb_data, target_size=(128, 128), class_mode=None, batch_size=batch_size, shuffle=False)\n",
    "    data.reset()\n",
    "    images = data.next()\n",
    "    img_ary = np.array(images.astype('float'))\n",
    "    rng = int(data.samples // batch_size)\n",
    "\n",
    "    for _ in range(rng):\n",
    "        images = data.next()\n",
    "        img_ary = np.vstack((img_ary, images.astype('float')))\n",
    "        #print (_)\n",
    "\n",
    "    channel_means = img_ary.mean(axis=(0, 1, 2)).astype('float')\n",
    "    img_ary = img_ary - channel_means\n",
    "    unlabeled_X = img_ary\n",
    "    unlabeled_X = img_ary/img_ary.max()\n",
    "    print(\"Exit Unlab_data_process\")\n",
    "    return unlabeled_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNLABELLED_PATH=\"./selector_output/unlab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Unlab_data_process\n",
      "Found 9852 images belonging to 1 classes.\n",
      "Exit Unlab_data_process\n"
     ]
    }
   ],
   "source": [
    "unlabeled_X=unlabeled_data_processing(UNLABELLED_PATH,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=model_k_fold.predict_proba(unlabeled_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_unlab=[np.round(predictions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_unlab=[round(i[0]) for i in predictions_unlab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions_unlab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
